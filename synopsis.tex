
\documentclass[]{article}
\usepackage[]{geometry}
\usepackage{biblatex}
%opening
\title{Towards formal theories of evolving referents to unfamiliar targets}
\author{Veronica Boyce}
\addbibresource{sources.bib}
\begin{document}
	
	\maketitle
	
	
	Language is really cool [citation needed] and supports a lot of human culture and tech by being an efficient way of conveying many concepts with precision. Lots of communication that ranges from stereotyped to open ended. If part of language is to transmit new ideas and support creativity, it needs to be extensible. Need to be able to use it for new things and with new people. 
	
	Some of these interactions may be ephemeral, never used beyond a single time and a single partnership, but others may become conventionalized/lexicalized in a community. 
	
	These reveal two critical phenomena: a) ability to refer to new things (that don't have established names) in a way another person can understand and b) ability to refer to a not-quite-so-new anymore thing in a way that is sensitive to the conversation history in some way, and eventually converges into just being a conventional name. 
	
	The expressivity is language is partially from this creativity in communicative usage. 
	
	So, do we understand how this works? In a word, no. 
	
	We have some experimental evidence, and collections of phenomena, but we don't have theories that make satisfying, testable quantitative predictions about what might occur. 
	
	Think about what sorts of formal (mathematical or computational) theories might we want, and what would it take to get there? 
	
	Why would we want this? One of the ways to tell that you actually understand it is to be able to make risky, testable predictions, and really understand how the pieces fit together. 
	
	\subsection{What might a formal theory seek to explain / why isn't there a formal theory already?} Setting aside issues around what is rewarded in the practice of science, it's difficult to make formal theories when the data is as rich as human interactions. Formal theories need well defined inputs, outputs, and functional forms. What would these be for communication data? 
	
	One could think about the generative model of human utterances, but the number of possible factors there is vast. One would need to define what the key phenomena are -- that are of interest in the real world and that appear across variations in paradigms. Once a phenomena has been determined, how to measure it? Language is rich and in an area like this where the versatility and creativity are part of it, a model to predict the actual words would be very difficult. 
	
	A few potential approaches exist. One is to look at the large scale of iterated reference and define phenomena in terms of categories: for instance, modeling how many words or turns are used, what categories of words, or other things like this. However, this supposes that either these properties are relatively invariant across "nuisance" variables (such as individual stimuli or partnerships) or that these variations can be averaged over or modeled as free parameters in a compelling way. This approach seems to lean more towards a statistical/descriptive model, rather than a really formal one. This is a more artifactual approach -- predict the transcript or the eventual outcome. 
	
	Another approach is to treat the questions of interest as: given a situation and a communication history, what properties is the next utterance (or exchange) likely to have. This lends itself more to fitting with a generative theory of how people select their utterances and actions. One can look at which words or concepts in a prior utterance are likely to be repeated. Some of the variation across stimuli and partnerships may be absorbed as variation in the conversation history -- it's possible that the dynamics are less variant, but that the smaller variations compound over the conversation history. 
	This N+1 from N approach neatly separates the question into two parts for the two critical phenomena listed above -- how to generate the initial utterance (or perhaps the first successful utterance) and then separately how to build from a previous (successful utterance) to a next utterance. This is a predict the a step in the process. 
	
	Another level, going further into generative parallels, it to bridge further with psycholingustic production and processing work, and model the incremental production or processing of these utterances. This requires a different kind of data, with measurements involving onset and reaction times or eye-tracking data to get at incremental elements. This is even lower level, predict the incremental process. 
	
	In the fully ideal world, we would have theories at all of these levels, and the theories would be compatible with each other. 
	
\subsection{here we will...} 
	
	In the bulk of this chapter, I will examine the existing "theory" options. To forshadow, we will not find any adequate formal theories. However, there will be frameworks that it might be possible to extend, as well as desriptive theories that point to phenomena that we may wish to capture. 
	
Pragmatic language use and the creative ways we can use language to communicate are rich. How do we tame this richness into measures and constructs that can be predicted and measured quantitatively? 

There's a lot of work on language use and particularly around repeated reference interactions with the same partner. These controlled studies reveal phenomena, such as partner-specificity, or reduction over repeated reference, or sensitivity to common ground. There are descriptions. But there aren't mathematical or computational models that make risky predictions to test future experiments against. 
	
The richness is part of what makes this interesting, but also what makes it hard -- the true generative model is complicated and requires all sorts of parts of language processing and production. Individual differences also seem to loom large. 

At the same time, there are more formalized theories from adjacent areas: such as theory of efficiency in psycholinguistics or rational speech acts. Are these extensible to the realm of more open ended communication acts? What linking hypotheses are needed to bridge theories with the data? 

Computational models could predict the bigger picture of phenomena without matching the process levels of the phenomena. Nonetheless, another place to look is at the micro level to see if theories of comprehension and production can lead to formal theories of the smaller scale, in even more controlled environments. 

To foreshadow the conclusion, a survey of available theories won't turn up theories that make strong quantitative predictions. What would it take to get to a place where formal theory development is productive? What existing frameworks are the most promising and what additional data and tools are needed? 

One preliminary question is one of scoping: are we talking about communication or language or face-to-face oral language? Different papers explicitly or implicitly consider different of these domains. Here, I will primarily focus on linguistic communication (writ large) with the understanding that some of the relevant factors are shared with other modalities and some arise from the richness of language-specific usage habits. % the pressures behind communication and convention-formation occuring in other modalities are also informative. However, the backdrop of linguistic habits and the extensive expectations it creates make languistic communication a particularly rich domain for pragmatics and convention formation. 

I will first briefly discuss the ``theories'' around referential communication with a partner (generally) and iterated reference, specifically. These are verbal theories that often lack clear scope, but they do reveal a set of phenomena, the quantity and extent to which we may want characterize and then be able to predict with formal theories. 

Then, I will discuss in more depth two potential approaches with formal theories: the framework of efficiency and the rational speech acts family of models, as well as what I consider the major hurdles to extending these frameworks to adequately explain iterated reference phenomena. 

The last region I survey is psycholinguistic theories, and how these might impose constraints or bounds on the formal theories. 

Finally, I will conclude with what I think some of the key open questions and promising directions are on the road to being able to develop formal theories in this area. 

\section{Communication}
The communication and conversation literature, especially with regard to referring expressions, has provided useful descriptive work, but is primarily made up of verbal theories that are vague, deterministic, do not make testable risky predictions, and use terms inconsistently. I briefly touch on these approaches, looking for parts that could be formalized and cataloging some of the phenomena that we will wish to explain. 

\subsection{mentalizing and non-mentalizing approaches}

A big question that comes up with conversation, and interactions between agents more generally, is whether and how agents are tracking other agents internal states of knowledge and how this factors into their interaction.

One broad approach to conversation assumes that humans have some sort of mental representation of other humans as agents with mental states. Within this broad school, there is variation in how these representations are implemented, how information gets added or modified, what exactly is tracked, and when representations (versus heuristics) are used. 

To reference shared knowledge, many use the term ``common ground''. In some cases, it is used to mean roughly ``things you think another person will understand and won't be surprised if you reference'' which is a useful pre-theoretic idea \cite{leung2023}. However, others use ``common ground'' in a theoretically-loaded way where there is infinite recursion and many things must be introduced into common ground via accommodation \cite{horton1996}. In practice, people often don't work through a lot of levels of pragmatic reasoning and instead bottom out after a couple [CITE FRANKE DEGEN], so it generally isn't important to distinguish knowledge types at deeper recursion levels mutual knowledge that both people know to be mutual. In fact, \cite{hanna2003} defines common ground as the ``mutual knowledge, beliefs, and assumptions'' held by the interlocuters. A useful idea out of this is that what people say may be influenced both by what they know and by what they think is shared knowledge with their interlocuter. In other domains, this is called ``givenness'' \cite{fay2010}.  

How exactly to populate these presumed knowledge representations has been a point of discussion that I will sidestep [cite CLARK USING LANGUAGE; horton1996]; however, if a formal model requires checking against what the other person is likely to understand or accept as an utterance, some sort of determination will need to be made (empirically determined or pre-specified). We'll also want this to be gradient. 

An alternative theory to mentalizing approaches called the interactive alignment theory attempts to explain how people can successfully collaborate on reference tasks without reasoning about each other's mental states \cite{pickering2004, gandolfi2022}. This work claims that the alignment occurs via ``priming'' is ``resource-free and automatic'', without providing a further explanation of what this means or how this is working on the level of processing, memory, and production\cite{pickering2004}. This failure to provide an actual mechanism was widely criticized in the commentaries. Given that humans reason socially about each other readily and from a young age, it's not clear what problem this non-mentalizing approach solves. 

The alignment tradition tends to favor symmetric tasks where the two interlocuters are doing the same thing (usually a `maze' task). In constract, mentalizing work often uses asymmetric director/matcher games, dating back at least to \cite{krauss1966}, where directors incorporate non-aligned feedback such as the response, or verbal backchannels such as ``mhm''. The interactive alignment account does not try to explain how conventions can evolve after they are aligned, whereas this is a core phenomenon that mentalizing approaches try to address. 

The mentalizing approach is more promising, although far from a formal theory. That is not to say that all tracking need be full explicit, but especially at a computational level, representations are a useful assumption, even if some might be implemented in associationist ways.  One kernel of reason is that usage increases the accessibility of a representation (\cite{macdonald1994}), or that the joint association between target, word, and interlocuter are encoded in memory. These processes are non-mentalizing operations that may lead to convergence in the use of words or constructions. A weaker (and more reasonable) interpretation is that production and comprehension share some things like the same memory and track usage statistics including is ways that are dependent on time and perhaps also activated by the context of conversational partner.  Could even be framed that expectations are shaped by the local context, including the conversation partner. 

\subsection{partner specificity}


One key phenomenon in reference games is the claim of ``partner specificity''. This is used both in cases when there are actually multiple partners and when each person only has one partner, but pairs evolve in language use time in different ways. 

The empirical evidence from experiments where there were multiple partners seems to point to people doing ``partial pooling'' over partners \cite{hawkins2021, yoon2014}. That is, a speaker A will show some variation in their expressions when talking to partner B versus partner C, but there will be some generalization between partners as well, so that A talking with B is more like A talking with C than D talking to E. 

Related to partner-specificity is the idea of ``audience design'' where speakers seem to be sensitive to the knowledge state of their listener and say things that are easy for the listener to comprehend. Confusingly, ``audience design'' sometimes implies intention on the part of the speaker and sometimes is used utterances are constructed based on what's easy for the speaker, and listener ease is a side effect \cite{horton1996, rogers2013, macdonald2013, horton2002a, horton2005}. This can be difficult to disentangle because speakers and listeners often have the same recent context, find the same things salient, and linguistically what is easier to produce is often easier to process. The study of audience design has raised on important topic of inquiry, namely, ``how do interlocuters split the communicative load with one another?''

Depending on the form of communication modality, there may be additional ways for speaker and listener to split the load beyond the degree of intentional audience design \cite{clark1996, fay2010}. For instance, a listener could describe what options they see or otherwise prompt the speaker. We might expect the load splitting to vary based on the capacities of the interlocuters (ex. a speaker might craft their utterances more when talking to a child versus an adult) and the capacities of the channels. 

Empirical work by \cite{yoon2018}, \cite{yoon2019}, \cite{yoon2019a} has started to characterize the space of how speakers behave with multiple listeners with different background knowledge states. The theories here are discrete such as ``aim low''. The empirical reality is that, at least up to a few listeners, and in these particular paradigms, speakers can keep track the correspondence between individual listener identity to histories and knowledge states pretty well. Speaker's can incorporate this with contextual factors that modulate task difficulty, \cite{yoon2019a} finds evidence for knowledge-scene integration.  It seems likely that tracking partner's knowledge states would eventually degrade as groups got bigger, but it's an empirical matter how large groups have to be before speakers weren't able to track individual performance and knowledge.

 Speaking with multiple people at once complicates the notion of partner specificity and audience design, as speakers have options for balancing knowledge states, such as using both the name that one person will understand and a elaborated description that will help another person get on the same page \cite{yoon2018}.

A full predictive model would need to account for more factors... TODO YOU ARE HERE  in general partner specificity seems like it will be constrained by factors such as working memory, task switching and bottom-up attention (might need to inhibit responses) 
TODO add aphasia stuff 
 This is probably a think that takes working memory, so might see declines if the people-context stuff gets too complicated, but it points to a lot of this being pragmatic and shaped by top-down non-linguistic factors about what to do. 


One way to disentangle may be to look at situations where what is easy to produce and what is easy to comprehend are different (CITE Ferreira commentary on \cite{pickering2004}). 

This is at group / modality intersection \cite{foxtree2013} looking at different remote communication methods; brings up the possibility of having different modalities at once with different people. In terms of backchannel -- expectations about usage should very based on ease, but also usage should be based on level of *need* to communicate / inverse level of understanding 

\subsection{"convention" formation}
Another key observation from repeated reference games is that partners form shared conventions about how to refer to initially-ambiguous targets. One observation is that conventions seem to be partner and context specific: changes in the speaker, audience members, or changes in the context can all license a new description \cite{metzing2003a, ibarra2016, yoon2014}.

The idea of convention formation is ambiguous between different levels of specificity: it could be a pact to refer to a figure as ``ballerina''; it could be thinking of the figure as a ballet dancer with a tutu (manifesting in descriptions that may not overlap lexically, such as ``ballerina'' and ``dancing in a tutu''); or it could be a general principle to describe figures in terms of humans in different postures. 
\cite{horton2002a} distinguishes between ``lexical entrainment'' when the same words are reused, and ``conceptual similarity'' when is a a broader similarity that does not repeat the same words. 

Another point of ambiguity is how intentional or binding this process is; the resulting short-form nicknames are sometimes called ``conversational pacts'' CITE. 

While the meaning of a description is not inherently related to its length; these two features tend to correlate in the empirical work around reference games, and thus ``reduction'' or the shortening of utterance is also used as a shorthand and measurement proxy for the semantic changes. It remains an empirical question whether the shortening of utterances and the convention formation are inherently coupled or merely occur together in the paradigms considered in the literature. These phenomena also co-occur and are sometimes inflated with partner-specificity, as in many paradigms, different pairs form different conventions. 

These are all interesting phenomena without satisfying theories. 

\cite{leung2023} points out that forming a convention can be thought of as preceding in two stages: first some referential expression must succeed in communicating the target, and only then can that expression turn into a more reduced form. Thus, there's both the question of how people initially decide what to say to communicate, and also, how people decide future utterances based on the shared history of a successful utterance. 

TODO dig into more: \cite{piantadosi2012} has me wondering whether conversational pacts are even real, or whether they are actually just contextual reduction is ambiguity and them peaking of the distributions in a slightly recursive way plus some recency effects and habit. This approach requires that speakers and listeners have similar models of language and the world at least in the relevant domain so that they can use contextual information to constrain the situation 

\cite{piantadosi2012} in line with RSA assumes that inference is cheap and that context and speaker goals are constantly taken into account 

Groups can be complicated \cite{guilbeault2021} looks at how different sizes of groups interacting over a network structure result in different category boundaries, where large groups end up more the same than small groups. Brings up questions about network structure and how we should think about networks even within groups where multiple people are interacting synchronously. Some of the other try to only allow lines between points -- either bidirectional is dialog or single-directional in monolog but there's still the development of shared stuff from the co-presence of listening to someone else even without direct interaction? 


\cite{hawkins2020b} How do you break symmetries in initial descriptions: there's prior variability across speaker preferences (they may each have a preferred label, but be unsure if others will accept it) and/or speakers may also not have labels and need to do some sampling. This doesn't account for production time course factors. 

this is a domain general model about abstracting over instances and forming conventions -- it is not particular to language, although peculiarities of language will also occur (and may have more levels to conventionalize on) 
\cite{hawkins2021} says there are 3 core cognitive abilities: the ability to represent that there is variability in other's lexicons; to coordinate via online learning; and to generalize across interactions ("partial pooling" model where updates both to partner and population) 

\subsection{Takeaways:} what information a person has and what they know about their partner are factors; mentalizing approaches are better. Would want to model how interactions are updates on the knowledge. A computational level model could be agnostic to whether partner-specificity is from intentional thinking about who knows what or just associations [amnesia work has some implications here].  

have evidence of some circumstances where partner specificity occurs, but a more full characterization may be needed. as well as potentially disentangling between intentions and speaker-ease motivated. 

Much of these phenomena are at the "overall" level, but could be brought down to the sequential level. 

The study of reference games has uncovered interesting descriptive phenomena but does not provide algorithmic explanations or clear demarcations for when they should occur. 

\section{Efficiency}
One unifying framework gaining traction in psycholinguistics is efficiency, the idea that language and language use is under pressure to support efficient communication by maximizing the ratio of relevant information transmitted to effort. Efficiency is thought to arise from trade-offs between communicative expressivity and learnability or easy of production \cite{piantadosi2012, kirby2015}. 

Many features of language distributions are argued to be much closer to the Pareto frontier than would be expected by chance. The distributions of word frequencies follow a power-law distribution, which \cite{zipf1949} explains in terms of a ``principle of least effort'', although note that power-law distributes are common across domains and generated by a variety of processes \cite{piantadosi2014}. Stronger evidence comes from the lexical partitioning of subdomains such as color, number, and kinship terms, where the distribution of systems falls on the frontier between complexity (number of terms) and informativity (how many bits each term provides) \cite{keysar2000, gibson2019}. Syntactic features of language such as harmonic word order or dependency length also appear to be optimizing for increased expressivity with minimized processing effort \cite{gibson2019, hawkins1995}. 
	
Efficiency pressures apply to language use the process, not language as a static code \cite{gibson2019}. Thus efficiency can be seen as imposing a joint constraint on the entire communicative process to minimize the total time and effort involved in going from an idea in one person's head to a sufficiently close idea in another person's head. Thus shorter utterances (as measured in syllables or clock-time) are not always efficient if they take longer to produce or parse. 

\subsection{reference expressions and redundancy}
Interlocuter behavior in iterated reference games may be informally referred to as efficient (in particular, the formation of reduced conversational pacts), but it isn't formally analyzed in terms of efficiency. However, other referential language use is, in the study of so-called  ``redundant'' color adjective use and other forms of so-called ``over-informative'' language use. As shown by the names, it seems like people's propensity to label something as a "blue cup" when there are not other cups around goes against the idea of efficient language use. 

Several issues arise here. Claims of redundancy or over-informativity require defining what is minimally informative which in turn depends on a commitment to a fully specified semantic-pragmatic system. For instance, if specificity implicatures are within the option space, are those calculated before or after informativeness is measured \cite{bergen}? Determining what is efficient requires not just analyzing phrases and their alternatives, but also production and comprehension time, which may be highly contingent on contextual factors and conversational history. 

One could sidestep the theory by empirically measuring the information content of different utterances by how they shift the entropy of the distribution of inferred meanings \cite{degen20200406}, but this does not scale up well. 


While ``redundant'' and ''over-informative'' are useful pre-theoretic terms, defining what a minimally-adequate description would be requires a fully-fledged theory of semantics and pragmatics. The naive use of these terms and the idea that utterances should have ``just enough'' information has inspired empirical research into what utterances people use. 

Claims of informativity are generally made relative to a truth-value semantics, disregarding salience or pragmatic enhancement of meaning. 

Problems also arise with for instance, specificity implicatures: does one calculate the informativeness of an utterance before or after the implicature \cite{bergen}?


The flip side of ''redundancy'' is ambiguity: many, many utterances are ambiguous. In general, strong contextual factors render the ambiguity a non-issue \cite{piantadosi2012}, but this means we can't judge language out of the physical and social context it is used in. 

\subsection{Takeaways:}
Some specific ways of caching out efficiency are testable, in particular for the what utterances listeners would accept or understand. There are enough possible linking theories it would be hard to falsify, so more of a question of what linking theory. 
Even without a fully formal theory -- could do some tests to constrain how calibrated people are and whether efficiency adjacent claims are even true. 
Theories are likely only going to be understandable with regard to a given semantic - pragmatics system. 

Might be some more theory neutral ways of trying to quantify information here in an empirically derived way. 

Mostly again at a high level, but has potential implications on any level. 

This seems like it would lend itself to a formal model: just list all the options and their corresponding levels of effort and informativity and see where reality lands. In practice, determining the options, their levels of effort, and their levels of informativity is a big problem that seems to require solving a bunch of other open quesitons. 

Efficiency is very hard to cache out in specific predictions because of the many time scales the pressures operate on: what's efficient for an utterance in isolation may not be efficient when considered over an entire life of language use. Thus, the efficiency framework is dependent on linking assumptions, and an efficiency approach could be seen as determining what link assumptions are needed to bring different phenomena under this umbrella, and then assessing the parsimony of the links.

Over the short time scale of one utterance to the next, the wider meaning of language (the prior) can at least be treated as static. There are still issues where What types of communication utterances are efficient should depend significantly on the context and communication channels. In particular, if interlocuters can interrupt with questions, or cut off a speaker by selecting a referent, then incrementally efficient utterances make sense \cite{gibson2019}. In other contexts, with lower feedback, it might make sense to more evenly distribute information content or rely on surer, but longer descriptions, if there isn't an option to add elaborations contingent on interlocuter behavior.

Efficiency also predicts that a changing conversational history will change the context and thus different descriptions may be efficient. This could operate both by increasing beliefs that a certain utterance will be understood (and this is contextually low ambiguity) or more generally by shaping the syntactic expectations, perhaps making it easier to produce and comprehend odder descriptions. 

\section{RSA}

RSA is an information-theoretic, computational framework for making quantitative predictions about pragmatic inferences in context \cite{goodman2016, frank2012a}. The basic idea of the Rational Speech Acts (RSA) family of models is to picture two interlocuters recursively reasoning about how the other would produce or interpret utterances, grounding out in a listener (or speaker) who behaves in a pre-specified ``literal'' way. Computational frameworks such as RSA provide a way to factor together different trade offs and determine their relative weights in a model. This set of models has been used to model some instances of ad hoc pragmatics as well as conventionalized pragmatic implicatures. 

This framework is usually run with one or two levels of recursion, where it tends to produce a reasonable fit to human experimental judgments, consistent with work finding that most people reason pragmatically at a low recursion depth TODO CITE FRANKE \& DEGEN. 

The basic idea of RSA is to specify some level-0 listener whose lexicon is specified in some way. From there, the speaker reasons about the listeners lexicon, and samples an utterance that softmaxes their utility function. Then, a pragmatic listener reasons about how to interpret an utterance by softmaxing a meaning given their model of the speaker and literal listener. This is a fairly basic framework, but many of the chunks here can get quite complicated when necessary. 

Different models in the RSA tradition incorporate different sets of components in the models. Models generally include a utility or informativity term that relates to how well an utterance resolves uncertainty in favor of the target referent.  It is common to also include factors such as the prior likelihood of referring to each target (salience prior) and some cost on utterances where longer or more complex utterances are penalized \cite{goodman2016}. Some models also go beyond informativity, incorporating options to infer the question-under-discussion CITATION or for speakers to balance informativity with politeness.

In some sense, a fully RSA model would incorporate all of these components and also infer their weights. However, for tractability, usually only those features that are considered relevant to the domain of interest are included.

Because of the flexible framework, it is possible to model many sources and levels of uncertainty, and then integrate out that uncertainty to make predictions, but also update on the sources of uncertainty in response to input. 

Perhaps the largest challenge to RSA models is the question of how to ground out the models in a ``literal'' listener or speaker. For the most part, RSA is tested in toy domains where the set of possible utterances are small and it is possible to enumerate a set of meanings. For instance, in some domains, a soft or continuous semantics is used to represent that some dimensions of meaning might be more strongly informative than others \cite{degen20200406}. This semantics supports the prediction of patterns of ``redundant'' color adjectives in referring expressions. However, soft semantics can run into conflict with compositionality: either every possible utterance must independently receive a degree of match with every possible object in the prior, or the prior needs to include rules for how to determine the match of a whole utterance on the basis of the match with each component. In a later experiment of \cite{degen20200406}, typicality effects made compositional semantics not work, but the utterance space was small enough that each utterance could be treated individually. 

 In less toy domains, there is not a satisfactory answer: some situations can be handled by empirically measuring likelihoods in an exhaustive ways, but this holistic approach is not compatible with incremental RSA or larger sets of utterances that require compositionality to be defined. In order to extend this model towards more realistic and open-ended scenarios, an important question to grapple with is what form of meaning (even at a computational level) will appropriately support pragmatic reasoning. 
 
This conundrum could be read as saying that one must define semantics before attempting pragmatics; I do not believe this, but prefer to frame this as a question of finding what model of semantics could be a linking hypothesis to allow RSA models to predict the patterns of pragmatic language use that are observed experimentally. 


\subsection{RSA approaches to reduction}
Scaling up RSA to handle reference games requires solving at least two problems. One is specifying a semantics system that can handle the abstract, metaphoric, and parts-based descriptions that are used -- this could be seen as the problem of accounting for initial reference. 

Secondly, one must explain how to go from one successful utterance to a different (often shorter) utterance. One RSA-style model that attempts to explain why multi-part descriptions are produced initially, but then later reduce to shorter descriptions is CHAI,  a framework to bridge different levels of convention formation \cite{hawkins2021}. TODO SAY MORE ABOUT CHAI In toy models of interlocuters playing a reference game with soft semantics, initial utterances use multiple properties to collectively increase the degree of certainty in the target. However, this successful reference then shapes the priors about the meanings of the words, until the degree of certainty afforded by only one word is sufficient. 

\cite{hawkins2021} separating the inference problem about what the other person's lexicon is (which is how they will interpret things in the moment, b/c you may have temporarily changed their lexicon) with decisions about what to say given that 

\cite{hawkins2021} points out that our models for communication and modeling the world and others need to be able to account for different people having different knowledge (including some tied to community membership or social role) and that vocabularies need to accomodate change over time and new things to refer to as the world changes. 

\subsection{Takeaways}
RSA seems poised as the closest theory that could be made to fit for a sequential model. Has the problem of open vocab, and of free parameter fitting, which reduce risk. At least a useful framework. 
CHAI while not a full explanation for the full patterns of real-world data, at least is a framework that actually explains why reduction would be optimal. 

would need some level of incrementality, or else would need to redefine the step where RSA operates. (this would favor greedy algorithm, but maybe that's okay especially if we take production constraints seriously?)
TODO CITE INCREMENTAL RSA

\section{Psycholinguistics}

Psycholinguistics imposes constraints on the algorithmic level of linguistic communication; however, determining the constraints requires understanding both production and parsing. Understanding the time course of production is particularly difficult to study. 

\subsection{Top-down or bottom-up?}
One major point of disagreement is whether production and comprehension are initially ``ego-centric'' or whether non-linguistic information, such as the perspective of the interlocuter, is exerting an top down influence from the beginning. For example, TODO cite Horton \& Keysar attempt to test this, but acknowledge that monitoring and fixing of the utterance, including pre-initiation of the utterance, may take into account the listener's perspective. %TODO say more about the egocentrism thing

Parallel to the egocentrism debate in production is an egocentrism debate in comprehension. Basically, the question is whether an intial stage of comprehension is egocentric, that is, not taking into account non-linguistic input about other's knowledge state or intent. This was primarily studied using eye-tracking as a proxy for interpretation, which is problematic as we don't understand fully what eye-movements mean TODO CITE JUDITH. 

In any case, \cite{keysar2000} argues for an initial egocentric perspective on the basis that people often intially look at objects that are good matches to a description that are not mutually visible with their interlocuter. They seem to be assuming that if people weren't egocentric, people should have very strong priors that interlocuters only refer to things that are mutually know, which is clearly false (why ask where something is if you can see it?). 

The counterpoint presented by \cite{hanna2003} is a constraint-based theory where many factors can play into comprehension, including working memory limitations. 

 This narrow issue raises larger questions about the relative influences of top-down and bottom-up factors in influencing language processing and production.Underneath the poor experiments and strong positions are a number of interesting and complicated questions. It seems clear that many factors can sway comprehension, including both "top-down" and "bottom-up" processes, and understanding the relative mix of these factors could be very interesting. However, it's a difficult empirical matter to disentangle all these factors and a number of nuisance experiment parameters especially when the measures we have are far removed from the constructs of interest. 

\subsection{Bounds on rational approaches}
Production poses a possible deviation from the idealized RSA models in that production requires the retrieval or generation of a potential good enough utterance in the first place. The difficulties of utterance planning may cause deviations from what information-theory would predict would be efficient, based on production biases such as easy first, plan reuse, and reduce interference \cite{macdonald2013}. 

Understanding the limitations on production is challenging: while it is easy to get examples of what utterances are produced, it is much more difficult to understand how the utterances were planned. One approach is to look at utterance production with and without time pressure and see how utterances differ, but this rests on (uncertain) assumptions about how time pressure changes the production process. 

Many theories such as RSA operate over a set of options, but don't provide answers to how the option space is created in the first place. 

\cite{heller2012} disagrees and says that speakers have a harder time than listeners b/c they need to model the listener (at least approximately) %TODO re-read heller

There's interplay between production, comprehension, and what the typology of the language is, that constrain the options for a bounded rational approach like RSA. \cite{macdonald2013} provides a functionalist account claiming that utterance planning is difficult, so speakers have biases in their production. These biases are easy first, plan reuse, and reduce interference. Taking into account both word-level and context-dependent factors, easy first seems to accord with memory retrieval theories. %TODO say more about other parts 
Under this model, these speaker patterns drive typology and the statistics of language, so comprehenders rely on these statistics of input to make predictions and do pragmatic interpretation. 



One issue with any `which came first and is the driver' arguments is that language is a cultural artifact and doesn't ground out in something external. There isn't an external set of statistics for the language system to be adapted to -- the statistics are also generated by the language system. Thus, as an empirical matter it is difficult to figure out which of comprehension and production is `driving' statistics, as both are under pressure to be calibrated to the another. This calibration is similar to the difficulty determining the division of labor in audience design, just on a different granularity of analysis. 

To avoid circularity, \cite{macdonald2013} claims that some of the production pressures are related to specific mechanistic aspects of memory and domanin general features such as saliency. %TODO reread macdonald and check 

one other angle on production is to look at disfluencies as a symptom of speaker needing to think (ex \cite{yoon2014} et al use this as a measure)

\subsection{Takeaways}
Hard to measure and hard to determine what measurements would adjudicate when it depends on thought. However, given sufficient data, models of utterance generation or incremental processing that tie to data on time course of production or comprehension (perhaps also with eye-tracking), could resolve a lot. There's room for fine-grained models, but we don't have the data. Nor do we understand the relationships between the stimuli or the search space for productions. 

THIS WHOLE SECTION NEEDS WORK!!
some takeaways related to how a different type of data is needed to understand these claims


	\section{What would we want?}
	
	I will not be presenting a formal theory here, because I don't think this topic is ready for one yet. I mean one could posit mathematical relations, but they'd be easily disproved because we don't understand the relationships between things. 
	
	It'll also a question of threading a needle between aiming to incorporate too many factors (being too ambitious) and too few (overfitting to too specific a scenario). 
	
	So, what exactly is the phenomena of interest here? 
	* how people describe images/objects/things where there is not a canonical or conventional name 
	* the dynamics of how this results in "nicknames" 
	--> why does it go through the intermediate steps it does; why is it partner specific; how understandable is it to the outsider? 
	
	(* seems like when there is a canonical / conventional name, one generally just uses that; if that doesn't suffice for contrast, add adjectives/modifiers that do. )
	-- > should be convergent with what happens in nameable places; also with coinages / evo of language and convention over time; should also be non-contradicting what happens in other communication modalities; 
	
	could simplify with given a starting point lexicon and a given history which might be null; what predicts what a person says. 
	
	"Person says" is super open ended and from data we have; there's lots of options; so there's a question of what the right things are to predict -- what is a useful classification scheme in terms of types of utterances or lengths of utterances. 
	
	There are a lot of potential knobs here -- could look at different situations etc. 
	
	Lots we don't know robustly like what are the parameters of the space under which reduction (operationalized somehow) occurs -- what sorts of stims, what sorts and sizes of constrast sets, what types of people, what communication modes? 
	
	While formal theories of iterated reference would not need to cover the larger issue of language evolution, there should be touching between theories of this and the larger scale issues that this can sometimes cause. 
	
	A lot of ways to slice or possible angles: when you don't have or can't produce words how to circumlocute to describe; partner awareness / cooperative communication / feedback; social habits / norms; could be an angle onto efficiency. 
	
	nicknames/ labels/ lexicalized units : not just metaphor: process of going from "bad but good enough" to "good"
	
	issues of calibration around what info is sufficient / per effort 
	
	Here are what I think are some interesting open questions relating to reference. 
	
	Open theory/predictoin question: One hole in the literature is a satisfactory explanation of the phenomenon of reduction that can make predictions about what words drop or are kept and what *rate* of reduction might be expected. Two starting points would be the CHAI model that gives a computational model for how longer initial utterances and later shorter utterances could be optimal \cite{hawkins2021}, and the idea raised in \cite{leung2023} that forming a convention requires two stages: some referential expression must succeed in communicating the target, and then shortening that expression into a more reduced form.
	
	Open empirical q: Related to reduction is the question of whether the reduction phenomena is efficient, or rather, what linking assumptions are needed to argue that it is efficient. Would shorter utterances perhaps be understood faster than the rounds where they are produced? Is the bottleneck on producing the shorter utterances? 
	
	Open empiricial question: One last area that lacks strong theoretic explanations is what happens in groups of more than two people. What would one need to add to RSA to explain multi-person dynamics? There are verbal theories about group interactions from \cite{yoon2018}, but other communication traditions don't cache out in clear predictions about group performance. 
	
	some sort of partial pooling 
	
	Some particular challenges for using RSA in practice are the various ways that communication is open ended: it has open vocabulary, open issues of compositionality, open number of turns, and (with spoken language) is highly incremental with options to take actions or interrupt mid sentence. As it becomes more open-ended, issues of how to consider alternatives and the psychological implausibility of actually considering all of them may come into play. A resource-rational approach that uses sampling instead of exhaustivity may provide a way of pushing this computational theory down into being slightly more concrete. 
	
	In terms of theories of interactions, I favor partial-pooling models that treat people as keeping some track of the individuals and situations they interact in while also showing some transfer-learning and generalization across people and contexts. How these two factors balance against each other is an interesting empirical question. 
	
	I believe that people are fairly sophisticated at modeling the knowledge states of their interlocuters, probably through some combination of explicitly modelling knowledge the use of good heuristics. Many of these beliefs may be graded, and many be on the basis of a heirarchical framework of what knowledge tends to be known by what categories of people.  Cultural norms around givenness are something that must be learned jointly with other norms about symbolic communication (primarily language). 
	
	Pushing down levels of analysis, resource-rational models should embody some of the constraints from feasible memory retrieval, production, and processing. Some of these will be from domain general processes (memory retrieval) and others may be about the language system in particular. 
	When considering repeated reference especially for difficult-to-name targets, I think there's a productive split between what is needed for 'first contact' -- establishing an initial successful reference -- and what drives subsequent references, where reduction and changes need to be accounted for. One key distinction that doesn't get explicitly highlighted much (but is brought up in some recent work, such as \cite{leung2023}) is that the process of `first contact' -- establishing initial reference -- is at least somewhat separable from the repeated reference / reduction phenomenon. They're probably not totally separate as the same systems will need to work for both, but there are probably peculiarities to the reduction part. 
	
	at a high level, want to be able to predict reduction curves! even just in terms of number of words, nothing that currently exists can explain why the rate and shape of drop-off is what it is! There's also claims that it's "efficient" or that partner specificity is "efficient" but the levels of partner specificity and so-called efficiency haven't been tested b/c we don't know how alternatives would have gone over!! could use more data on what the curves are like -- so you have more to fit to. 
	
	at a mid level, want to predict one utterance from the previous -- may be able to do something RSAish where we think about how much info each part gives (could do some empirical derivations). would need to come up with the right set of operations / primitives to get away from the openness
	
	Another piece is that reduction isn't just about shortening in the number of words or concepts used; it also tends to follow a stereotyped pattern, where more abstract, holistic "names" stick, while descriptions that may be more concrete or describe the image piece-mail tend to drop out. Why is this what happens?  Is there a way to model the initial semantics of these pieces and of the updating such that this pattern of what sticks is predicted? 
	
	An additional complexity here is that the end point can vary. Usually holistic or analogic descriptions win out \cite{clark1986}, but this isn't absolute. Sometimes groups may use something strange and even something that is on a lower-level or on a meta-level as their referring name, and for them, this can be effective. So to fully explain this, we need to take into account the path dependency for how reduced utterances evolve, and potentially also account for issues such as people's relationships or humor value. 
	
	A fully satisfying explanation that doesn't card code some preferences may need to refer to production and comprehension factors. 
	
	Groups! Its easy to get lost in studying reference games, but the full phenomena of describing initially non-named things takes place beyond dyads in a variety of situations (CITE ROBERT NETWORK STUFF). There's lot of claims made about how face-to-face dyads are more natural (which tends not to be substantiated), but it's an interesting question how people's capacities for communication are used across a variety of settings and modalities. Good theories will also be able to explain variation based on factors like communication modality and group size. 
	
	CITE 20 questions with nature paper
	
	also question of how to extend RSA to multiparty? 
	
	\section{How will the rest of this thesis move us in that direction?}
	
	TODO WHAT'S THE CAUSAL MODEL
	Overall, it seems that despite much research, there are still questions about what exactly are the phenomena to be characterized and how far they extend. Over this thesis, we provide data in various direction that could be useful for theory building. 
	
	many of the pieces could use more data
	
	a) a broader look at when reduction and semantic convergence patterns occur: how extensive is this phenomena by looking at larger groups and different modalities. This evidence contradicts many of the verbal theories that privilege dyadic communication and other things. 
	
	b) we dig into the data from a in an attempt to look at the stage to stage evolution of descriptions (TODO gotta do this research)
	
	c) developmental trajectory
	
	d) what's the processing look like? 
	
	? e) ? do we include any AA or game-theory? 

\end{document}