\documentclass[]{article}

%opening
\title{ARRR: Why most theory is this area is weak and wrong}
\author{Veronica Boyce, local cynic}

\begin{document}

\maketitle
% Next steps are to pick a hunk of this and organize it!
\section{TO explore further }

* how to bridge information theory and syntactic/habitual expectations for language form 

What's the usefulness of hedging in communication? For reference not for politeness -- what purpose does it serve, when does it create better results (is it comprehension or production side)? 

Why don't things reduce extremely sharply after first success / speed of refinement

What are processing/production ways to get at cost and truthness of descriptions and also how are they sampled? 

whats the memory component over time? like if you asked people who had played the game to then later name all the figures? would they use the names their group had even months later? 

%TODO not sure what if anything to say about Eliav

%TODO so I read hockett and hockett and don't think it has usefulness here? 

%TODO look at kronmuller and barr MA (again?)

%TODO not sure there's anything relevant to say about khani et al 

%TODO not sure if there's anything relevant to say about turner

%TODO consider adding some cites from yoon2014 to the list, especially around constraint based 

%TODO zaslavsky -- confused on the details, but something about what efficiency is from an information-theoretic perspective 
%TODO figure out what my not how memory works coment on Clark chapter 4 was about 

%TODO consider constraint based models 

%TODO look at \cite{clark1986} more, both notes and original text


What's the SOTA in language production modelling? Utterance planning? 

Constraint based accounts of production (and comprehension) see \cite{hanna2003} highlights for suggested readings

maybe grice

forward search \cite{heller2012} since they're pretty reasonable about the idea of name versus what properties to include 


see various yoon and brown schmidt for highlighted citations to read 

\section{Goes somewhere but I haven't figured it out yet}
\cite{clark1996}
discussion of the difference between natural signs and signals -- which isn't a distinction I especially buy, but does raise questions over how we determine whether something was an intentional signal from an agent or not 

raises lots of philosophy questions about whether intent is required for something to have meaning and questions about the developmental trajectory of all of this 

\cite{murthy2022} on color chip associations with words. This uses a strongly uncertain framing where we have priors over word-color associations. How does this map up to words where there's more structure to the prior in some complicated way. Also evidence that one can learn a lot from feedback, including things that may have had really shallow priors. 

Connection to CG: but how deep does recursion go? Could look at the Franke \& Degen for some of this, but generally seems to be only a couple stages on this recursion, so maybe recursion for CG is also only a couple stages -- not sure how strong the connection is here
\cite{bergen} on how to get matching between costly utterances and rare meanings which is needed for efficiency


\cite{fay2010} "minimized collaborative effort" (will need to go back and check but notes on: what sorts of interaction define something as being communicative intent, what sort of feedback) use of "givenness" as probably slang for some sort of cultural CG. getting to be more adjacent to some of the network-y stuff that Robert does (but see notes if needed) 

\subsection{Robert stuff} 

\cite{hawkins2020b} one question here is how are people so flexible in how they describe things. There's some ad hoc pragmatic reasoning going on, also people have learning mechanisms and can adapt to their partners. In the world, there's new things to experience, so we need ways of using language in new ones (that's just part of the necessary language capacity). There's also going to be new people and contexts, so we have to operate under uncertainty -- we don't know exactly what is shared. Also there's person to person variation in semantics / how they use words. And we can use feedback (verbal and not) to adjust. 

Pragmatics isn't some special thing, these are actually pretty universal. We're closer to living in tangram world than we think. (Except that we usually don't even have such a nice closed class of alternatives) 

Arbitrariness and stability on the semantic side 

\cite{hawkins2020b} How do you break symmetries in initial descriptions: there's prior variability across speaker preferences (they may each have a preferred label, but be unsure if others will accept it) and/or speakers may also not have labels and need to do some sampling. This doesn't account for production time course factors. 

\cite{hawkins2021} introduces CHAI, a framework that tries to bridge the levels of conventions from partners to networks and societies. seems generally right; need models that can explain both partner specificity and community scale conventions

this is a domain general model about abstracting over instances and forming conventions -- it is not particular to language, although peculiarities of language will also occur (and may have more levels to conventionalize on) 
\cite{hawkins2021} says there are 3 core cognitive abilities: the ability to represent that there is variability in other's lexicons; to coordinate via online learning; and to generalize across interactions ("partial pooling" model where updates both to partner and population) 

need to fill in the gap for what is happening in the conversation level synchronic interaction that repeated many many times across the community then drives the observed diachronic change 

\cite{hawkins2021} points out that our models for communication and modeling the world and others need to be able to account for different people having different knowledge (including some tied to community membership or social role) and that vocabularies need to accomodate change over time and new things to refer to as the world changes. 


need some sort of structured heirarchical representation of word meaning (on the part of the other person) -- how do we represent word meaning and in particular the sorts of word meaning profiles that another person could have 

what's the ability to compound utterances? how do we do informativity measures? and how does this play out with more real world stuff? there's a toy model that I think is broadly correct, but you also start running into syntactic expectations for natural language which in the individual instance run counter to pure efficiency and informativity (what if there isn't a noun) 

\cite{hawkins2021} separating the inference problem about what the other person's lexicon is (which is how they will interpret things in the moment, b/c you may have temporarily changed their lexicon) with decisions about what to say given that 

points to an open question about what realistic priors would look like here

partial pooling approach to generalizing across populations (but this is not when all were in a group together) 

a direction we're not really considering here, but this generalization should be expected not just across partners, but also across contexts (which will tie back into usage -> structure and non-arbitrariness and efficiency arguments )

another piece that isn't super relevant here, but context matters for what conventions will form since what level or levels will be informative (again ties in with communicative utility and thus efficiency )

should expect lateral inhibition from successful matches -- similar to pragmatic reasoning but we think about each choice being also a choice against the other options 

note that the GD of \cite{hawkins2021} has reasonable things to say about children and how they might have weaker priors and be less attuned to how much content is needed to communicate 

\subsection{List of sources of disagreement}

Memory format 

Ease of social reasoning (note that social cognition says that people are pretty good at a bunch of this and that it appears early in development and is quite advanced and may be a major factor in being human, so theories that posit that this is super hard and requires tool-level thinking are probably confused, or at least not getting parsimony points from this)

Explanation for reduction

Ego-centrism / time course of listeners restricting to CG (if that even makes sense)

Monolog / dialog /etc 

Is language privileged and if so, what specific instances of it

Rules v probabilistic

Is pragmatic inference an always thing? Role of context in linguistic interpretation (yeah, probably always there, any restrictions would have to show up in processing and would be hard to measure? could throw around surprisal with non-linguistic context, but idk if that's been studied) 

general tension between people who want a set of absolutes, versus gradients

how difficult doing theory of mind is (and under what circumstances)

time course of incorporation of interlocuter specific information 

interpretation of eye movements

what appropriate levels of naturalism are

what work is on the speaker v listener

"""reference diaries"""

terms I don't like: common ground, entrain, aim low v aim high v aim average

Who adjusts how much: do speakers design and/or do listeners accomodate




\subsection{mush about audience design that I don't know how to classify}

\cite{horton2002a} Here they make the assumption that audience design is only sometimes needed and is osmething that is uniformly an increased in the verbosity. So what's the default. You're always talking to someone, except/even if talking to oneself right? 

\cite{horton2002a} usefully points out that partner specificity should occur mostly in cases of low codability, which I'd elaborate as being low codability in context since context will matter here!

Audience design has big definition problems is that different people use it to refer to different things Sigh. 

Two possible components could be  responding to interlocuters cues about their understanding (if they express confusion say more), and making good guesses about what they will need to understand based on shared history and inferring from their demographic features (these are different!) 

\cite{horton2002a} useful distinction between lexical entrainment where it's the exact word used again versus conceptual similarity which doesn't need the same exact words, and can generalize (double check if this is in there are my thoughts) 

is incorporating things initially contributed by listeners a component of audience design? 

\cite{horton2005} makes the (reasonable) claim that conversation partners act as cnotextual cues for retrieval of associated information (shared history stuff, not demographics) presumably via episodic memory 

\cite{horton2005} how well you can design utterances from your audience depends on how well you can know what the other person knows

\cite{horton1996} makes the correct point that a result does not allow us to infer intent -- the appearance of audience design could potentially be arising from something else and its just hard to know

\cite{macdonald2013} talks about audience design in the form that things may be beneficial to comprehenders, but still be driven by the producers needs, in that they produce what they can produce with fliency and without difficulty (rather than let the conversation get slow or repetitive) 

To some extent the idea of audience design is about how the labor of the conversation is split between participants and what the processes of these are -- right we assume it's a join thing, that there's cooperativity in having the conversation at least 

\cite{rogers2013} points out that audience design can be used as a descriptive "it looks like it's tailored for the audience" without actually committing to any intent on the part of the speaker to tailor for the audience 

\cite{rogers2013} raises the point that it may be difficult to tell what is strategic and what arises as a result of incremental social interaction (although it's really unlikely to be fully bottom up) It's also possible to have a strategic incremental approach (this is what's probably happening) where there is integration and monitoring of the other people. This like everything else fails to explain reduction. 

\subsection{Misc: modularity of language}
different approaches make different claims and assumptions about whether what the thing that's interesting to study is language specifically or communication, with language being the specific case study that comes with a lot of conventions. 

We're coming down on that language should be considered as an instance of communication, and thus, the general things should also hold for other forms of communication (drawing, gesture, etc). But, there's a lot a lot of conventionalized stuff around language in particular that makes it a) an interesting area to study and b) have some complexities that may not exist as much for other modalities. 

\cite{clark1996} at least brings up for me what delimits language from other forms of signalling

This is at group / modality intersection \cite{foxtree2013} looking at different remote communication methods; brings up the possibility of having different modalities at once with different people. In terms of backchannel -- expectations about usage should very based on ease, but also usage should be based on level of *need* to communicate / inverse level of understanding 

channels may differ in their richness and efficiency (some of this might also be habit over different channels)

\cite{hawkinsa} Drawing convention formation over repeated drawing shows some similar patterns to language reduction. These similarities point towards the processes of interest such as reduction and alignment with partners being general and agnostic to the medium of communication, although of course they play out in different ways depending on the affordances of the medium. For both drawing and language there are culturally built up habits about how things are typically represented. Drawing, some is still cultural, but more is human universal based on perceptual stuff, rather than language where the conventions are more language specific. 

\cite{murthy2022} on color chip matching -- there's something to learn about convention in domains other than language. Theories should make at least not wildly inconsistent predictions about those areas. 

\cite{pickering2004} seems to at least implicitly think that language is modular in that their theory is built around aligning on levels of language; some of the commentators (such as krauss \& pardo) point out that language is not this modular or automatic in that it can be influenced by non-linguistic cues and background. They also point out that language can be used strategically. 

\subsection{lack of supremacy of certain form of language}
Some of the language studiers also make claims about certain forms of language (oral, face-to-face) being the more correct ones to study, with implications that being the easier, truer medium. 

I disagree -- different modalities and channel set-ups have different affordances. Implications will vary from different set-ups, but even for "naturalistic" there's a whole lot of not face-to-face these days. 

\cite{clark1996} argues that face to face conversation is the more natural (chapter 1) and thus that all other situations of language use will be more challenging. This goes against idea of transfer learning or ability to adapt to other situations. Notably, in the current world, a lot of linguistic communication, even informal communication, takes place not face to face. there's also a strong focus on dyadic interaction in particular, unclear if dyadic is actually a more natural or common situation that others.

\subsection{joint action}
\cite{clark1996} while not the central interest here, theory should be continguous, so we should consider how to consider language when used in conversations with more limited entities. People do talk to their pets and their babies. People also talk with automated system that use language, and now talk with AI chatbots. Are there something totally different? or are there gradients of different actions with some being more prototypically joint and cooperative than others -- at least for conversation with a toddler, it's a conversation, but this suggests that there isn't a clear delimiting between joint actions and not b/c of semi-cooperative situations where one party is not very good at cooperating (pet, baby, AI). 

(generally, insert railing against over splitting approach here)


\subsection{"""Alignment"""}

\cite{eliav2023} (maybe?) brings up the idea that conceptual alignment could be one to one or it could be a wider thing that displays transfer learning (concepts are on different levels) 

not sure where it goes: but there seems to be good explanations for why using multiple chunks might be good b/c uncertain if other person will understand (although see open questions about how true this is) 



\subsection{partner specificity}

\cite{hawkins2021} for explaining treating different partners as different contexts and explaining the partial pooling and gradient into group conventions. partner identity is part of the (episodic memory) and is tied to the usage, but as it gets encoded more generally in non-episodic may be the process by which it becomes convention level 

\cite{yoon2014} also supports a partial-pooling type model, in that there's some sensitivity to context and who said what, but also there are still cross-interlocuter recency effects

\cite{yoon2014} point out that representation and use are key (idk what my notes meant by that) 

\cite{yoon2014} posits a pretty high level of reasoning in that disfluencies (fairly low level linguistic signal) are explained away by the presence of a co-listener. How far does this go in terms of what sorts of knowledge about the other listeners position will or won't do explaining away? Also at some point we expect to allow that the speaker might be using imprecise heuristics and we can't form strong expectations b/c they might just be messing up. 

\cite{yoon2018} sketches some different options -- we expect that this is actually going to be a lot more gradient than that, and it would be cool to have a mathematical model of some of this

\cite{yoon2018} raises question of when speakers are able to track different states of listener knowledge? both in terms of group, but also in terms of other cognitive load and performance/speed pressures 

in general partner specificity seems like it will be constrained by factors such as working memory, task switching and bottom-up attention (might need to inhibit responses) 

task demands and goals could interfere

Seem to be questions both of how much information is provided total and in what order its provided -- see \cite{yoon2018} on the interpretation of elaborations versus replacements -- giving name + more might be a way to accomodate different knowledge levels and get people closer to the same page 

\cite{yoon2019} there may be task demand issues in terms of what the function is for how much to include everyone (based on task and incentive might be better or worse to slow down and include everyone versus not) even if these aren't explicit, might be transfer from other situations. 

\cite{yoon2019} when there's feedback there's option space for speaker / listener effort splits how does this work 

sort of relates to the ibarra, but \cite{yoon2019a} points out that there's knowledge-scene integration where interlocuters take into account the physical perspectives and contexts of their listeners and how that interacts with the knowledge states. This is probably a think that takes working memory, so might see declines if the people-context stuff gets too complicated, but it points to a lot of this being pragmatic and shaped by top-down non-linguistic factors about what to do. 

\subsection{Interactive alignment goofiness}


Newest iteration is from \cite{gandolfi2022} brings up a lot of "control" and monitoring and comparing 

unsure how one is predicting the others utterance without ToM -- is this is behaviorist thing? is the other person not being represented as an agent? or is this just a levels of analysis claim? (representation in general are weird and hard to work with)

I'm confused about this -- and what meta-representing is doing ; feels like a high level description that doesn't have a lot of specific explanatory power

is this really just another "we don't like studying that" -- focuses here a lot more on repair and meta stuff 

\cite{hawkins2021} has a good take-down pointing out that priming approaches can't explain reduction b/c why wouldn't it stay the same? and can't explain when the listener doesn't talk but provides feedback in another way (ex. mhm, correct selection, etc) 

contra the interactive alignment, even \cite{krauss1966} thinks that both verbal backchannel and correctness feedback should play a role in shaping whether descriptions reduce or what gets used next 

\cite{pickering2004} says it's going for mechanistic, but doesn't provide satisfying mechanisms. 

Why do they think that mind modeling is hard? I don't think that's consistent with social cognition work -- the mental modeling doesn't have to be explicit or fully recursive, but people tend to actually track others pretty well. 

\cite{pickering2004} seems to be trying to solve a problem that doesn't exist -- in that it's assuming that the obvious mental modeling thing is costly and it's not clear what evidence they have that this is a problem that needs to be solved. Agreeing what words mean for the purposes of conversation can't be that hard (after all people do learn new words somehow)

Also it's possible to model someone else's lexicon as being different from one's own. 

\cite{pickering} it's also really unclear what the extent of their claims are -- this is a super motte and bailey situation where it seems like they're claiming that this "priming"/no-modeling this is the general case that happens all the time, but then they can retreat to the claim that mental modelling does happen, but that it isn't the only thing. 

\cite{pickering2004} makes the claim that this is priming and is "resource-free and automatic" but they don't explain what that means, so I hardly think this counts as a mechanistic explanation. Seems to be a much stronger claim than the more likely to be true idea that usage increases the accessibility of representation (a la easy first from Macdonald) or that the association between the interlocuter the target and the words are encoded in the memory. But priming is a phenomena that also needs to be explained on the memory level and so isn't an explanation. 

\cite{pickering2004} don't spell out what is happening and instead use terms like priming or alignment 

A weaker (and more reasonable) interpretation is that production and comprehension share some things like the same memory and track usage statistics including is ways that are dependent on time and perhaps also activated by the context of conversational partner. This is much more broadly agreed to. Could even be framed that expectations are shaped by the local context, including the conversation partner. 

\cite{pickering2004} also seems unable to account for times when people communicate and there isn't echoing of the words, and instead just assent. This is perhaps due to them studying a different task that has a different collaborative structure, but it does limit their explanatory power as they seem to make predictions that run counter to the evidence (although see motte and bailey). 

\cite{pickering2004} some of this may be that it's very unclear what level of analysis they're on -- "priming" feels like a verbal theory,but they seem to be acting like they're on a pretty implementational level. Lacks explanatory power where they basically decide other situations aren't basic enough. 

\cite{pickering} as the branigan and markman commmentaries each points out, an especial weakpoint is trying to get this to prime all the way up to the situation model which seems extra extra implausible. 

also generally, many commmentaries provide good take-downs of \cite{pickering2004}

\cite{pickering2004} reads like a behaviorist in treating mental modeling as the abnormal work around and wanting everything to be automatic. The idea that mentalizing is super normal is pointed out by many commentators including schober. 

in their response, \cite{pickering2004} really dig in on the alignment thing and defend that their situation level model is really just like the others (now what this is I don't understand, but it seems to be leaving language specificity behind). Bring up the idea of "implicit CG" again without defining it. 

Look into more, but \cite{pickering2004} in their response seem to suggest that efficiency is self-driven -- I mean they really don't have a good explanation for reduction (or anything else) 


\section{Interpersonal / Common ground}
\subsection{CG}

\cite{horton1996} says some things that I disagree with about exactly what CG is, and buys into other absolutist claims about what isn't or is allowed

\cite{horton1996} is interested in the processing timeline incorporation of CG -- raises a lot of questions about what the current production literature is, assumes that social context is complicated and costly to put online, really bizarre discrete time step assumptions; doesn't clearly specify what the alternatives to using CG are; there's also issues where some of what looks like audience design/CG might actually be various heuristics or speaker-dependent functions and it's hard to do that (caching, yknow) 
evidently, \cite{hanna2003} has a less crazy definition of CG ? 
\cite{fay2010} calls this being "given" within a pair or community 
Common ground is frequently alluded to, but inconsistency in how the term is used poses problems. It is also usually framed within a non-probabilistic framework. Here I attempt to lay out what sort of common knowledge system is useful. 

As a first gloss, common ground is things that person A expects person B to share and expects B to not be surprised that A thinks it's shared. 

Much ink has been spilled distinguishing common ground with the infinite recursion of knowledge from some spy game. People are pretty bad at holding onto several level recursion. For this theory, it seems worthwhile to think about what forms of private knowledge are common and which are the really hard to keep track of kind. 

It seems reasonable to model people as being able to have up to a couple levels of recursion (with increasing difficulty as recursion increases) or as "many" as that the information is shared and known to be shared. (Aka, most people aren't spies and aren't playing Clue) Whether or not these things are actually shared this way does not matter, the question is a) what the psychological reality is and b) what's reasonable for models. 

Common ground doesn't actually exist (no platonism here) so it always needs to be considered from the viewpoint of a person who can have (implicit) beliefs about others that are used in producing or interpreting utterances. 

This common ground thing is used both at the level of lexicon / interpretation of symbols and also about the specific knowledge about the world and the current state. And also about the temporary ad hoc conventions being built up! 

Whatever this is, it needs to be flexible -- people don't need to be sure that someone else knows. In fact depending on alternatives, backchannel options, stakes, etc, it may sometimes make sense to use descriptions that you don't expect someone else to get, followed by elaborations, because if they do get it, it's the most efficient and this isn't one shot, it's repeated/incremental/interactive. 

\cite{clark1996} contra Clark, many interactions don't require CG -- it's possible to have an interaction with someone who is a lot less aware of things, such as in talking with children, where it's still a conversation, but there may be a lack of cooperation and a dearth of shared territory. 

how to mesh idea of CG with "curse of knowledge" -- CG can't be veridical -- it's a construct and the priors might be messed up by own knowledge (cf people who think that others are modeled on self) -- generally  people may not update knowledge well enough / have incorrect levels of certainty. 

Also for curse of knowledge when people know they're failing to explain, might be a production issue, albeit probably on a conceptual level. 

\cite{clark1996} chapter 4: really one of the populizers of common ground, but issues
- what level is this claim at -- is this a super high level claim or is it about how people approximate what's going on
- what's the developmental trajectory of a proper shared basis
- this blurs various sets of expectations about interlocuters, including language use, other shared cultural stuff, etc etc

we have a lot of expectations about how humans work -- unclear how much of this should be included, because some of it is like an expectation that the mechanism of joint attention (looking at person and looking at object) will make it mutually known. 

what levels of intentionality are assumed here? 

CG and coordination -- what's up with asymmetric exchanges such as adult-child interactions 

you can act like you have CG without having it based on heuristics (and this is probably what kids do, see also their common failures on this) 

even without specific relevant knowledge, \cite{garrison2022} brings up that when there's shared social history (even not about this topic) that makes for more efficient label sharing ; alcohol also gets better labels for strangers, but unclear on mechanism. 

May be cultural mores about how to refer to things or how clear to be that run counter to a pure efficiency model. (Especially if there's perceived social cost to asking questions, might not be on equilibrium). 

some uses of common ground and conversational pact are used descriptively as a sort of people as contexts claim \cite{leung2023} rather than as a commitment to the theory 

\subsection{Why common ground is dumb}

The idea of common ground assumes that it's a thing that exists. That isn't true but also isn't relevant -- what matters is what one person thinks is sufficiently mutual between them and their interlocuter. The depth effect of common ground is rarely relevant, but people can be mistaken about what is in common (ex. you think it's in common ground because you told it to them, but actually they forgot). Also, as will be discussed extensively, this isn't all or nothing -- I can have uncertainty over who knows what, or think something will be understood because it's generally known by people without specific reason for the individual. 

The question of what are your priors or their priors and how tailored are they to the current context and in particular how much theory of mind and perspective taking (aka recursion) there is, is relevant. And that's vaguely what's being said, but with the mechanisms probably mostly wrong. 

One issue with distinguishing what shapes utterance production is that we primarily have access to the outcome at the end, which mostly isn't what anyone is arguing about. The questions are about what the process is for generating that utterance and what sorts of things are how prominent in the process when. This leads to lots of hand-wavy models about how language is produced (again, hard to model well since we don't have access) and flimsy linking assumptions with behavior. 

One assumption is that telling someone to respond faster (time pressure) will change the production pathway/process in a way that limits later steps more than earlier steps. Clearly, time pressure (and other resource constraints) do affect production, but how is going to be a lot more complicated, especially given the lack of good production model. 

Horton \& Keysar are trying to adjudicate between: initial planning takes into account common ground, and initial planning does not taken into account common ground, but monitor \& fix does (and does so pre utterance initiation sometimes) 

Everything is probably more continuous than these things posit. It's a probabilistic world of bounded rationality and heuristics to fill in gaps. 

egocentrism: what is seen as egocentric behavior could be either a) not caring whether the other person understands / could be referring to a thing, b) mistaken beliefs about what the other person knows , or c) satisficing under limited resources producing either (which isn't really a third option) 

the people who call it egocentrism don't call it that when people make mistakes about what is in common ground. But as \cite{hanna2003} point out, you can never actually know what another person knows 

the ways things are said to get into common ground aren't consistent with various general assumptions that people know things (like default assumption of sharing the same language, or knowing basic facts, etc) or inferences (went to same college, is a local, etc). 

common ground is mostly useful when discussing spying -- something like expected to be comprehensible to listener based on some set of things makes more sense 

\subsection{theory of mind and social representations}
One question is about the partner specificity of representations and how they may bleed together

This also interacts with other things such as ToM all of social cognition and the nature of memory representations 


\section{Communication writ large}
\subsection{Misc: Convention}

Another place there isn't a clear line is between conventions and non-conventionalized coordination. 

Can get coordination from game theory, from probabilistic stuff, don't need to resort to common ground

conversational pacts are not just partner specific they are context specific \cite{ibarra2016}. if the context changes in a way that affects task goals or information needs, people will change their descriptions and there isn't a problem. whatever the expectations are about continuing to use the same ones, that's not absolute (and is probably the product of something else) where usefulness is a factor 

\cite{krauss1964} hypothesizes that there will be convergence and frequency-dependent modification where things referred to more times will get more convergence / shortening
 \cite{leung2023} has an implicit pattern that in forming a convention for a low-nameable thing there's two factors, getting some referring expression off the ground, and succeeding at reference and then a pattern of reduction after something has worked. This seems like a really useful high level framework. 
 
 not really sure where this goes: \cite{metzing2003a} for how much conceptual pacts stay, I think there's a question about whether what is being studied here is lexical level or conceptual level. Presumably there's some gradient and there could be entrainment on different levels. 
 
 Whether it's weird when a new speaker uses an existing convention depends on how strange the convention was and on things like evesdropping, their common knowledge background (maybe that's just what all doctors call it...), community membership
 
 Also this whole literature around new speaker / old speaker breaking pacts is a lot less robust than the amount of weight that's put on it. Wonder what the p-curve on the meta-analysis is ... 
 
 \cite{piantadosi2012} has me wondering whether conversational pacts are even real, or whether they are actually just contextual reduction is ambiguity and them peaking of the distributions in a slightly recursive way plus some recency effects and habit. This approach requires that speakers and listeners have similar models of language and the world at least in the relevant domain so that they can use contextual information to constrain the situation 
 
 \cite{piantadosi2012} in line with RSA assumes that inference is cheap and that context and speaker goals are constantly taken into account 

\subsection{Misc: Communication}

\cite{clark1986} proposes a try-repair-accept framework where reference expressions don't have to be certain to succeed and can often be present with hedged language indicating their tentative status. If they don't work, there's room for back and forth repair, before a description is accepted by both parties. Acceptance can be implicit in the form of action, or verbally expressed. 

\cite{clark1986} because communication is a joint activity, there is this possibility for repair. In the production of unrehearsed speech, if planning falls through or doesn't stay ahead of production, there may be fillers, restarts, mistakes, etc. But this may still be an ideal efficient path in expectation because it makes use of the ability of the listener to step in and backchannel. It is possible to have production strategy where you keep saying things until they choose the referent. Wouldn't need to calculate a level of informative/overinformativeness because it'll be empirically determined when they choose the thing. 

\cite{clark1986} idea of mutual acceptance is very formal way of putting it -- often won't need to be explicit and joint attention or assumptions of understanding could also work (possible to adapt to systems that due to affordances have different costs of actions and this will effect thresholds -- do you expect verbal response, or are you on a "it's good unless you hear otherwise") 

\cite{clark1986} Will have to look this up again "how does an appeal to prior acceptance work"



\cite{clark1996} idea that language (use) is a joint action

\cite{clark1996} useful take aways: - some communicative acts require a signal from the other person to close the loop (not necessarily communicative) or else there will be continued follow up (or it’ll get dropped) and the type of things that close them vary (basically people look for feedback signals from others)

- there’s on some level some considerations of who interlocuters are and what they know

- people taking into account current circumstances including prior responses ; in general tends to be some sort of monitoring if interactions for various signals when engaged in interactions / joint activities.






\section{Psycholinguistic considerations}

\subsection{Assorted psycholing}
\cite{bybee2006} possible that encoding linguistic events into memory and then retrieving is a mechanism that results in efficiency on a language over time scale? (look back at text for more) 
refers to frequency effects -- perhaps some of these are actually predictability effects to link to a more information-theoretic approach 

Unsure how these few things fit together, but a few things to consider.

Where the bounded part of bounded rationality comes in -- we expect RSA-style to be a good high level model, but it's not an implementation and is agnostic about what parts are cached and at what level things are cached or short-cutted. 

Another issue is production (in terms of alternative generation, if we think RSA): (leung has a good list for this) As a speaker, one has to come up with a description at all, that description needs to be a good enough fit for the target, and then it also needs to not overly fit competitors. But generating these good enough for target descriptions is non-trivial in some circumstances, and this isn't well-modelled that I know of. How do people generate descriptions in the first place? 

Because this is language, there also seem to be strong habits or expectations about form. Like, there's a level of syntactic well-formedness that's expected, even if it's in some sense inefficient. What's the psycholinguistics here? Is this actually what's fastest to produce because of the habits of mapping to speech? Fastest to interpret? (Syntactic expections for there being a noun come into play in \cite{degen20200406} --> can't say blue without saying pin as well) 

\cite{hanna2003} Looking at the time course of using the speaker's context in processing. Bug questions around whether people are initially egocentric in their interpretations. Makes some weird assumptions about what's required to take other's perspective, on a more mechanistic level, without paying attention to the possibility for some cached heuristics. 

What would an egocentric perspective really be? Language only exists in relation to others (mostly), so how do we think about word meanings in the absence of considering the speaker (clearly there is some way of doing this, "generic speaker", whatever, but I don't know if it's well understood) there's probably also an interesting developmental component -- what's the interplay between language development and ToM development? 



\cite{heller2012} says that speakers have a harder time than listeners b/c they need to model the listener (at least approximately) 
re production should look into what the current consensus around production is 


\cite{horton1996} is interested in the processing timeline incorporation of CG -- raises a lot of questions about what the current production literature is, assumes that social context is complicated and costly to put online, really bizarre discrete time step assumptions; doesn't clearly specify what the alternatives to using CG are; there's also issues where some of what looks like audience design/CG might actually be various heuristics or speaker-dependent functions and it's hard to do that (caching, yknow) 

For the whole egocentricity timeline stuff, there's a big question of what things factor in how much when and how much that gets pushed around by other constraints such as working memory etc. There's multiple dimensions here (not to mention the many nuisance ones), and we don't have many points of experimental evidence. 

\cite{keysar2000} makes assumptions that we know a lot more about processing and how to interpret eye-movements than we in fact do. Seems to assume that people should have absolute assumptions that what is communicated about will be mutually know via being in mutual view. This is dumb and in real life is never going to be that strict -- it is possible for people to refer to things that are out of their sight at the moment and even when you don't know that the know about it. That needs to be within the considered distribution. Seems to be that the questions are about the relative mix of top-down and bottom-up processes when searching for a visual referent. 

\cite{keysar2000} seems to imply that audience design means that listeners don't have to do work (maybe I missed something) 

\cite{pickering2004} the Ferreira commentary beings up that there are speaker / listener asymmetries and that these may have audience design / work splitting consequences as easy to produce and easy to comprehend are not always the same thing 

\subsection{Production}

\cite{macdonald2013} thinks that production in particular is very important. This gives a functionalist account focused on production as the driving thing that then informs both comprehension and typology. Claims that there are three biases in production: easy first (taking into account word level and context dependent retrieval factors, seems relatively consistent with memory stuff), plan reuse (which is the explanation for structural priming) and reduce interference. The bigger claim is that utterance planning difficulty is the driver here, and that comprehension interpretation is just relying on the statistics of the input from speakers. 

This has got to be cyclic over time, since unlike things with external statistics for humans to learn from, language is a human product and the product and the production and processing and none of them externally determined. Unclear how to adjudicate over which pieces are the driving bottlenecks and what is adapted from there. 

Claims that production is winner take all (\cite{macdonald2013}) not sure how to think about that really 

\cite{macdonald2013} also claims that some of the production stuff is related to pretty specific mechanistic memory stuff (which doesn't seem implausible, but I don't know the memory literature well) basically it hopes to ground things out with appeals to domain general features of saliency to avoid circularity about what is easy to produce 



\section{Efficiency}

Various areas of psycholingustics are tied in with a paradigm of efficiency as a high-level descriptor of a pressure on language and language use that occurs on multiple time scales. 

For most of this, has to work backwards from the observed features of language and language use, although at the small time scales can be experimentally observed as well. 

Question is basically: what pressures could explain the properties that are observed. Efficiency is basically a trade-off and most theory say that it's the result of two somewhat opposed pressures although what exactly the pressures are varies. 


There's a growing consensus that various aspects of language over varied timescales, from evolution of specific language systems to language use, display properties that are parsimoniously explained by efficiency. What does efficiency mean? Something like close to optimal in the trade off between informativity and processing/production. We could see this as a trade-off between different pressures (and that's probably how it developed?), but it could also be close to optimal for how quickly the information content (thought/idea) that a speaker wishes to communicate is conveyed to the listener's thoughts, taking into account that this includes mapping from thoughts to language, producing the language, time to transmit the sound (how fast the speech production is), parsing the signal back into linguistic units, and inferring meaning from that. There's all these (somewhat overlapping) steps, and especially for common messages and key information content, language is far above chance. This could be seen as trade-offs in various things, or as trying to minimize the process subject to some standard of close-enough on the meaning front. 


Lots of questions about efficiency (see also the notes taken below already)
What are processes that shape efficiency (especially in real-time production)? How does efficiency shape interpretations? There's the long-form for rare bias, but that requires pretty well-formed alternative sets. 

What do we really mean with efficiency? How does it play out on different time scales? What expectations does the overall efficiency framework impose on communication and reference? 

This also gets to ideas of informativeness. And if you get into informativeness and the interaction with relevance. 


As an overarching thing, efficiency is hard to prove or disprove in part because it relies on a lot of linking hypotheses for how it should play out in different domains. But it has parsimony and explanatory value across a lot of levels. It's a good fit to the data, even if the realms for experimental tests are somewhat limited. 

As Zipf points out, it's important to be clear about what the imagined optomization function is (when thinking at a high level), or what the pressures are that create the constraints at lower level. Efficiency depends on what the time scale that's being optomized for is -- someitmes more prep / longer will pay off over repetition (and you don't know the future, so probabilistic)

Related to a lot of efficiency work is trying to figure out what the pressures on language are -- like what's the stuff that's being optimized for here, but you have to back it out from behavior. 

\subsection{Time scale: languages}

Zipf is an early proponent of language (the system / artefact) being shaped by some efficiency pressure ("principle of least effort"). He points out that if you're maximizing something, you need to get it all into one equation, you can't optomize two things at once (unless you specify how they trade off with each other, but that's jamming them into one equation). His evidence for this is ... lackluster, and there isn't an attempt at looking at other possible explanations for the data, but his work did inspire others to do a better job at looking at this. 

\cite{zipf1949} if we're being generous it's an early example of bounded rationality / resource rationality in terms of trying to find tradeoffs 

\cite{zipf1949} makes some observations about the tradeoffs between the number of signs and how much meaning each sign has; although this seems to be an ahistorical account that isn't covering historic changes in word meaning or the processing the drive the equilibria we see. claims about numbers of meaning are presuppose a knowledge of what counts as a separate meaning. Makes the observation of a power law distribution (which led to Zipfian being a synonym) Number of meanings versus usage -- also more to the point there are strong claims here and sloppiness with regard to units of analysis and when there's intent and what level of analysis and plausibility it's going for. 

Railing against zipf stuff: \cite{piantadosi2014} we observe from zipf that the rth most frequenyt word occurs roughly 1/r of the time (inverse proportion to rank) this is a power law distribution 
Zipf claims that this is due to his minimum effort theory (see zipf). But there are many, many processes that could and do produce power law distributions. So the fact that this is power law distributed is very weak evidence since there's many alternatives for generating this pattern. There's actually a richer pattern in language in terms of both small deviations from this pattern and the fact that this pattern shows up in domains where you restrict to words with approximately the same denotation (but strongly varying levels of tabooness/slang/context) -- this isn't explained by Zipf's claims because it's not a mapping of how commonly different meanings need to be expressed and what words are chosen for those meanings. A lot of claims in this area aren't very testable, or aren't very cognitively reasonable, or don't even explain this narrow cut of the data. Rather than focus on this one feature, it makes sense to look at broader patterns that need to be explained, for instance by efficiency based explanations. 

\cite{kirby2015} talks about how language has a combinatorial structure at multiple levels and that this combinatorial structure is key to be able to talk about new things (which is important because of the non-stationarity issues) 

\cite{kirby2015} key idea that combinatorial and efficient are in tension because a fully regular system will make everything very long

\cite{kirby2015} many language systems and subsystems that we see seem to be near the expressive/ compressive trade off space; claim that this is because of tensions between compressibility (important for learning) and expressivity (communication) 

\cite{futrell2022} From an information theory perspective, trying to back out what functional constraints language is operating under / what constraints would explain language. What is language optomizing for and what pressures are driving that optomization? 

On a language level, both semantic categories and syntax (and potentially things like phonology as well) seem to be closer to what optimal would be than would be expected by chance. While we can look at this and it looks optimized for efficiency (in different parts of the communication sequence, so addressing different potential bottleneck points). Some of this is that commonly conveyed meanings have short ways of saying them (they have words aligned with those meaning levels) instead of either requiring additional modification (increasing speech time and processing time) or unecessarily overspecifying (increasing processing time). Another component is that syntactic properties such as harmonic word order seem to be good for allowing listeners to generally parse things reasonably. 


\subsection{Time scale: conversation} 

\cite{kemp2018} on semantic typology. When measuring informativity and what information loss occurs as a result of communication, we need to do this relative to what is important in the situation to communicate. With their kinship examples, the question is how often things like older and younger or gender of siblings are relevant versus just the fact of siblinghood. Over time depending on how often these sorts of factors have to be communicated (which the claim is, may be socially specific) that will determine what distinctions are lexicalized versus not. This is determining where on the frontier different languages end up, but they all tend to be not too far off from the frontier. Depends on how often things are referred to and how specific it needs to be in those cases (basically how close the contrast set is). 

\cite{hawkins2021} points out that increasing efficiency (at the level of interactions) often isn't so much about saying less as about being able to express more and more complex ideas in the same amount of words/time 

\cite{gibson2019} ideas of communicative efficiency, complexity, and learnability- these are all factors that probably shape things. These are going to be context dependent on the usage situations. For in-context efficiency, if there's a lot of uncertainty on what will be an adequate description and there's a strong backchannel or possibility of incremental, then should try things because there's the opportunity for clarification. 

\cite{gibson2019} for various syntactic things, including information locality, these might count as efficiency under the idea of making things maximally easy to understand. This is then going to be how fast can you sufficiently accurately construct the tree from the string, which will be how fast you can convey meaning, but on the comprehenders side? 

seems like what's being minimized is the time to communicate including the production and comprehension processing times (expression length isn't the only thing that's minimized)

Over time an arbitrary form-meaning mapping becomes systematic which then means that the mappings aren't arbitrary because there's embedded in this system and so couldn't change unilaterally. 

\cite{gibson2019} it's hard to measure things like communicative utility which makes it harder to instantiate some of this theory (unclear which things are my thoughts in response and which are brought up in paper) 



There's a question of what the microlevel processes are the drive efficiency, and this is where thinking of things in terms of trade-offs comes in. If something is too hard at any point and causes that to bottleneck it's bad, but things that ease whatever the bottlenecky step are will be more successful and those more widely adopted. There's also the possibility that sources of flexibility become grammaticized as they are reinterpreted (cf. Hawkins), that is soft processing constraints that lead to things like heavy-NP shift may then become seen as part of the grammar by frequent use and then the less frequent parts don't get used even in the specific (rare) situations where they (in isolation) are processing-optimal. There's pressure to grammaticize either from learning or from making parsing easier because fewer options? 


On a more relevant level to present interests, speakers and listeners converse in situations where the language is approximately set in stone. There are expectations about what words mean and how syntax works. Given those inventories, speakers and listeners get to make choices about how to interlocute to efficiently do cooperative mindreading. There's a lot of possible levers here: 

* what wants to be communicated comes with a certain level of precision (or lack thereof). in reference, you just need to get the right target given context. when describing an event, there's parts of it that you want to convey, but it's not recreating everything
* depending on syntax and word order constraints (that is, expectations), different languages lend themselves to pointing out different things. We might imagine that in some circumstances one might want to say isolated words out of order, but this would be pretty unnatural and so incur processing costs on the listener end. Possible also on the speaker end producing it, if planning takes longer. If the channel is artificially limited in a game (or telegram) maybe it's worth this trade off, but in everyday speech, the speech length may not be the limiting part. 
* in most conversational environments, there's the option for the listener to respond or otherwise for feedback to be given. thus, there's the possibility for stopping early if an utterance is sufficient or continuing if it is not (because sufficiency is ascertainable from the listener response either verbally or behaviorally -- i.e. reaching towards the wrong thing). How much this feedback is possible varies, but when it is, it makes sense that utterances can use this. Thus, the most efficient path may be to try something and add more only if needed. (This goes well with not necessarily having absolute knowledge of the interlocuters mental state. )

* over the course of a relationship (or conversation) the conversation history shapes expectations which will allow for different descriptions to become efficient -- either increasing belief that something is enough to try, or making less syntactically normal / highly reduced things seem okay (and thus not incur processing cost?) in context. 

How to deal with that a lot of the time people are joking or whatever? Still trying to convey meaning (even if complex, or meta-linguistic meaning)

\subsection{Time scale: utterance} 
\cite{rubio-fernandez2021} looking at language efficiency and incrementality -- brings up questions of how immediately things are parsed (linear v heirarchical) and suggests a conflict between efficiency and informativity -- although really efficiency is informativity x briefness right? 

\cite{rubio-fernandez2021} when looking at incremental efficiency; from an informativity perspective in the moment you're going to get a clash with well-formedness and syntax some of the time -- how does that resolve and why does it resolve the way it does (is it production / comprehension that means those aren't actually efficient, because there's more to life than phoneme counting) Seems to put the brunt of the weight on the speaker for doing design for the listener 


\subsection{informativity}

This relates to efficiency/ informativity / and RSA: \cite{piantadosi2012} discussing the role of ambiguity with is relevant to the question of what sort of utterances to make in reference in games or conversation. Descriptions are not going to be unambiguous without context and they might not even be very good descriptions so one is going to need context and a shared history to disambiguate or shift the probability weights more strongly for conventions

\cite{piantadosi2012} gives a useful contextualizing of zipf and cites up more recent work that is more empirically rigorous for power-low patterns. 

Various opposed forces that are supposed to yield efficiency: \cite{piantadosi2012} says it's clarity (low ambiguity and high recovery of signal) that is opposed with ease (of production / processing prefers a smaller set of items) 

the communicative context should play a large role is determining how much signal is needed for clarification and thus shape usage 

This is not a well-defined area -- we talk about descriptions being sufficiently informative, but not overly informative, but to make those calculations, you need to have already posited full on semantics and a lack of noise in the interpretation channel. Whatever level of pragmatics you allow, then screws this because now your just barely informative are in expectation overly informative since others can be pragmatically enhanced. Better to explain all of this in terms of RSA where you can deal with it flexibly and not need to specify what the baseline is. 

\cite{baumann2014} argues that in RSA-style environments, speakers often produce "over" informative expressions, and they make the claim that it's because calculating the implicatures is costly.  do listeners ever misinfer b/c of overinformativeness? what's the downside of being "over" informative except from an efficiency stand-point? how do we even define "over" specification, since this is always going to be in regard to a level of inference and a level of probabilistic certainty that we're okay with. 

\cite{bergen} on specificity implicatures will also raise questions here, since do you calculate informativeness before or after the implicature? 

From efficiency if production time speed if a big factor, then it may be overall efficient. 

\cite{degen20200406} on informativity as being the relevant thing *not* some technical definition of overspecification 

idea of "pragmatic surprisal" 

\cite{heller2012} on claiming that overinformativity is a thing, but this is deeply problematic. Also mild overinformativity is unlikely to throw the listener?, also questions of how to interpret things like name + descriptions that aren't efficient in the moment but may make things more efficient later (by linking to a name) 


\subsection{functionalism -- probably part of efficiency }

\cite{hawkins1995} not sure where this goes, but a functionalist approach. This is an approach that assumes a pragmatics as a last layer view rather than a it's all pragmatics all the way through view. 

Efficiency means that things that are more common should be faster to process and this is a joint constraint on the processing system and on language (language might be the faster evolving part here, so that may make language adapt to usage and ease of processing constraints)

From an ease of parsing perspective, it's easier to have less in the buffer at once and to avoid ambiguities if the parts that determine the structure of the parse are closer together (assuming free word order). This sometimes gets grammaticized, and then because if the grammaticization based on say the common heaviness pattern, there will then be exceptions. 

Might be conflicts between balancing syntactic weight for ease of I think comprehension and not production (contra Macdonald?) and information theory. 




\section{RSA / probabilistic models }


** Intro to RSA ** 

I'm coming out in favor of RSA-style systems -- there are questions about what the right sorts of models and levels of recursion are for the model and there's separately a question of how these get implemented. 

\cite{frank2012a} This is an information theoretic approach, with the goal as to how to quantitatively predict pragmatic inference in context (that's the big goal). Question here is how to do this with less clear referents like abstract things. Features that matter are the contextual salience (although this is sometimes ignored) and a surprisal-based informativeness measure (how much the utterance reduces uncertainty) which could either be empirically or semantic-system calculated. Lots of work is about how to scale this system up to things that eventually come into contact with interesting interactions. Although also may be able to account for some conventionalized pragmatics too. 

\cite{goodman2016} RSA is a computational framework for making quantitative predictions about pragmatic behavior. Variations on RSA pull out different features could be at play -- in some sense a full model would have to include all of these, but many are particularly relevant in different situations and so get modeled in those cases. 

** Advantages to a probabilistic approach

One point of distinction for a lot of theories is how they handle uncertainty on the part of interlocuters. Many do not and instead contort themselves to accomodate the fact that people make (in these models) assumptions about what others know based on appearance/etc (assuming member of speech community, assuming general knowledge ). But then they also have to specify accomodation and repair mechanisms to deal with when things that were treated as being commonly known where not known by the interlocuter. Especially in reference to hard to describe objects, people may not always describe things in ways that they "know" the other person will understand, which is harder to explain under these stricter models. 

Probabilistic models allow for uncertainty over what other people know, and what others think words mean, etc etc, and thus some of the accomodation is more built in. 

Gricean maxims provide some descriptive coverage, but there's often (perhaps always) conflict between them and none of them are well enough defined to specify what counts as "relevant" etc. 

** One big question is what sorts of semantics are needed to support the model working. 

One central issue that comes up when discussing linguistic communication is the semantics-pragmatics divide (if you believe in that) or how word meanings are represented. Words certainly have conventionalized meanings (that's how we can talk to people who we haven't talked to before), but there's questions about what the representations are and separately, what an appropriate way to model them is. 

Is literal semantics a useful model? Clearly lots of formal semantics isn't psychologically plausible, but some of the approaches could be useful simplifications for plugging in to RSA. What literal semantics will support RSA to explain different things? Like, what if we treated semantics as another Marr Level 1 component of RSA. 


** Some of the uses have been to explain conventional pragmatic implicatures. 

\cite{bergen} is mostly trying to explain a very particular set of implicatures raises questions about what types of semantics are a reasonable substrate for RSA. (see notes for details about options for semantics) 

Things that at one point may have been pragmatics become entrenched by usage (possibly within partner specific ad hoc interactions, but certainly over time for a community) 

** Some have been to explain reference expressions. 

\cite{degen20200406} in choosing a referring expression, some trade offs between cost of utterance and informativeness (also many, many things no where near the frontier, just for the record) 

\cite{degen20200406} uses continuous/soft semantics to explain how redundancy can be useful

could look at \cite{degen20200406} as asking the question: what's a system of semantics and RSA pragmatics that would generate and explain these observed phenomena (what are the constraints or processes that could lead to this output) 

idea of soft semantics is necessary for dealing with tangrams where truth value semantics is unlikely to cut it -- then the specificity and truthiness of the description bits is what's competing 

** All raise questions about how to account for all the sources of variability. 

Some key questions here are: what are the alternatives to an utterances? What the literal semantics being used? What's the belief/likelihood representations? 


there's a conflict between compositionality (which \cite{degen20200406} get around by using whole phrase semantics) and incremental RSA / ordering constraints for cross-linguistic --> points to a tension on how to do both

Again a big question is how to ground the semantics of the literal listener -- what usage is based (and what is pragmatic enhancement), and what sort of semantics even works. Truth value doesn't work all the time, how to introduce uncertainty there? 

Some RSA models incorporate cost terms -- abstraction of various things, which is probably related to efficiency pressures (or is modelling the causes of efficiency pressure) 

Questions of how to model uncertainty over word meanings and other parts of interlocuter states (what do they know if this is not mutual knowledge) or their goals

\cite{goodman2013} this paper again raises the question of what's the right literal meaning to use  -- is it some sort of context free expectation? 

How much do extra-linguistic, domain general factors affect pragmatics? Priors? Seems like there's a fine grained interaction between knowledge and implicature (not really sure what I meant here)

\section{What I think is true}

We need theories that can fit in with theories at different levels and from adjacent fields. We should be clear about what level and scope of explanation a theory is trying to provide. 

I think resource rational approaches are likely good. At a Marr level 1, some sort of RSA approach allows probabilistic integration of different goals and linguistic and non-linguistic information. Some of the real challenge here is that there are a lot of factors that can come into play depending on the features of the situation and the interlocuters. Eventually, you'd want a theory that would be able to explain all of them all at once, but that's fairly intractable. Instead, makes sense to stay aware of that goal and slot in different pieces as needed, as steps towards building a whole. 

Some particular challenges are the various ways that communication is open ended: it has open vocabulary, open issues of compositionality, open number of turns, and (with spoken language) is highly incremental with options to take actions or interrupt mid sentence. 

There are constraints on the optimality and these are also important to consider -- some of these may be memory constraints or processing constraints that are domain general about how veridical memories are and how much we know about and can track situations and interlocuters. If we start getting to implementational levels, or even just resource-rational levels, will need to start accounting for how many things can be integrated at once. 

Another source of constraints for implementational levels is how the language production and processing system works -- what possible utterances are and how they are produced and how things can be structured are constrained by language. 

Broadly think that efficiency is a driving factor and seems to have broad explanatory power, although we have to think about how it plays out in any particular situation. 

For communication, we could see efficiency as manifesting in trying to optimize bits of (goal-relevant) information conveyal per unit time. Conversation is a complex process so there's a lot going on: there's a mapping of goal to relevant information (which might be lossy), the speaker then has the access words to produce (which may not be easy for descriping low-nameability referents), and produce them. The comprehender then has to parse the string, interpret it (along with whatevever pragmatic integration is required) and then take an action. Often efficiency is established on the basis of how long the words are, but in some cases saying more words may be efficient if it reduces production costs or comprehension costs (making the task easier, or making it easier to understand by being nice to the parser). 

We don't have computation models for how to cost all these pieces against each other, so for now it's hard to verify these things. 

While here I'm following a high level probabilistic approach, it has a lot of similarities to the constraint based approaches in considering trade-offs between a number of soft constraints. In this way, has a lot of properties shared with cognition generally. 

Don't distinguish hard lines between "normal" and abnormal situations -- communicative and linguistic skills can transfer to new contexts and depending on the contexts, relative weighting of different factors will be different, but those are just the extremes of the same distribution we are constantly in. 

When considering repeated reference especially for difficult-to-name targets, I think there's a productive split between what is needed for 'first contact' -- establishing an initial successful reference -- and what drives subsequent references, where reduction and changes need to be accounted for. One key distinction that doesn't get explicitly highlighted much (but is brought up in some recent work, such as \cite{leung2023}) is that the process of `first contact' -- establishing initial reference -- is at least somewhat separable from the repeated reference / reduction phenomenon. They're probably not totally separate as the same systems will need to work for both, but there are probably peculiarities to the reduction part. 

Factors are very unlikely to be all or nothing. A salience prior makes sense, as a priori there may be expectations that an interlocuter will refer to something mutually visible for instance, but the prior space also needs to include that they're referring to something you can't see, or making a joke spouting nonsense. These low-probability outcomes that need to be allowed pose a challenge for absolute system, and also seem to mean that priors need to incorporate everything, albeit at extremely low levels. (Need to remap 0's to something tiny) 

Considerations from audience design literature: generally pretty reasonable, communication is in some high dimensional space and there are a lot of potential influences. 

``partial pooling'' model: some sensitivity to context \& who said what, but source memory is hard. Also, there are recency effects \& some expected transfer/universals (learning language) 

Distinction between elaborations (common with a new listener) \& totally new ideas (rare)

From audience design: speakers take into account the whole set of listeners who are present -- tend to tailor so that all could understand, but use meanings/shorthands that knowledgeable ones will recognize (=> many possible ways to accomodate multiple listeners, perspectives, also consistent with a name \& describe that's good for future) 

performance is constrained by realities of task switching \& memory and interference (so some performance things that don't seem ideal may be from those things) -- bounded rationality 

speakers can also take listener context into account (at least when known) but again probably bounded by working memory

shared social history makes for efficient label sharing 

A few directions that seem useful:
* start at RSA and attempt to scale up to accomodate conversations (tackle the open ended things)
* try to figure out what the bounds from memory / processing / production are and push a model down to a more resource rational / sampling based implementation
* something something extend theories to how groups work?


\section{meta}
general meta-theory: when something is a new area, easy to make over-simplifiying models of “here are the two or three ways it could work” this is useful, but as a field matures, important to accept nuances.

useful sketch of some options versus reality / full messy space of options

Stronger if you have gradients (ideal if you have math model and can do something quantitative)

\subsection{ Levels of analysis and implicit versus explicit knowledge}
many theories are loose on what level of analysis they are addressing things out, and if there posit representations whether they mean that those are actually in the mind somewhere or whether it's a model that might not be actually possesed by people in that form. Also how much there may be a layer of approximation over everything. 

\subsection{experimental space and messiness}
20 	questions with nature. 
the experimental space here is quite huge and many of the experiments lack in generality and rigor (it's hard not to, especially by the standards of the day). Thus, a lot of the theoretical claims, while possibly useful and true, are not well supported by the evidence presented. 

In the start of an area of inquiry, it makes sense to use blunt tools and try to categorize things and limit the "messiness" in order to get any sort of traction and be able to start a theory (at least, this is a common approach, unclear if I actually think that it makes sense to have as much early theory). As evidence accumulates and a landscape of results starts to take shape, this stops cutting it. It becomes clear that there obviously is complexity and ignoring that complexity no longer suggests the right next experiments. Over-simplifying assumptions are common in the theory here, which may have been useful and necessary, but increasing maturity of an area comes with expectation that more be explained, and more quantitatively. 
\subsection{Constraints from other disciplines}
We're not making a theory of everything here, but if there are going to be appeals to parsimony, they need to take into account all the other stuff that will perforce be going on outside of this. Thus, if some things already are needed or well-explained that's to be  taken into account, especially with regard to domain general processes. 

In particular, reference contacts other disciplines in the forms of social cognition, since this is a social interaction and involves tracking other's understanding and knowledge states, and memory, since it involves remembering things. What we know and belief from these disciplines will need to match. Thus, if something is common and considered a solved problem, we can't appeal to it being hard. On the other hand, memory is decently understood and so we should reject models that posit implausible memory systems or memory retrieval operations. 

Models may be skimpy on these fronts if they are high level, but that should be explicit. 

Other evidence from language use and perception/ categorization boundaries may also come into play. 

\section{Open questions that aren't satisfactorily answered}
\subsection{Unsatisfying ?: Explaining reduction phenomena}
This is one of the big puzzles -- it makes sense from an efficiency standpoint, but most theories are better at explaining why you should form and maintain a pact and less good at how it should crystallize into something shorter (in this sense, changing). 

Robert's point about having multi-symbol things and how usage then refines the meaning us one approach. Still a question of modelling the rate of drop-off. 

Also seems like there's some probabilstic path dependency here, possibly related to relationships or humor value

we may be able to identify what types of descriptions/utterances tend to work out (holistic/analogic ones re: \cite{clark1986}) but they aren't absolute and sometimes ones that picked up on very different features are ones that end up working -- there's a wide variety of what can work + some strong preferences (so probably some distribution and priors) 

\cite{hawkins2021} has a toy model mechanism that incorporates great lexical uncertainty -- you use multiple parts to refer and then over repeated correct use, will jointly update both parts until the utility of both for reinforcing meaning is outweighed by cost. There probably are other factors and specificity issues, but this is the only theory that has a computational model of why you should see this change after success has been established. (going to depend on the semantics that are used) 

\subsection{Unsatisfying open ?: Groups}


\cite{yoon2019} how much do you think of the group as a unit, versus as a set of individuals (obviously this is going to change some as group size changes, but what are the patterns) 

\cite{clark1996} focuses very much on dyadic interaction, and is in the vein that multi-person is serial dialog (verify this last part). 

how to extend ideas of joint action to accomodate multiple people? (re clark1996). 

One question is how you extend RSA up to multiple people -- how to do an utterance designed for multiple people? I guess you could represent separately and fit whatever joint function for optimizing that you want? 

groups allow for more complex structures. \cite{fay2000} tries to look at whether 5 and 10 person groups behave more like dialog or serial monolog (to the group) -- I think it's unclear that this is useful -- how do you define interactions / how do you usefully measure commonalities. There is a suggestion that there may be a transition point in how groups behave, possibly around 10 people, although context and goals seem very likely to push this around 

how to account for side conversations or parallel tracks which are available in some modalities (and not others) also other mediums such as fora / discord / slack have different affordances for allowing branching and parallel things that might be better for groups 

Groups can be complicated \cite{guilbeault2021} looks at how different sizes of groups interacting over a network structure result in different category boundaries, where large groups end up more the same than small groups. Brings up questions about network structure and how we should think about networks even within groups where multiple people are interacting synchronously. Some of the other try to only allow lines between points -- either bidirectional is dialog or single-directional in monolog but there's still the development of shared stuff from the co-presence of listening to someone else even without direct interaction? 

There should be some gradients in terms of these forms of more indirect interaction that aren't explained by dyadic models because the mechanisms that are claimed to support the partner specificity don't cover it (for in part being too low level?)

\cite{hawkins2021} for groups, how should we think about the pooling and updating based on the group -- have a different type of somewhat indirect evidence about the group members, although how indirect it is may depend on group size and structure and dynamics 

it's really unclear how \cite{pickering2004} would accomodate three people in their priming approach. can three people act as two people? 


\bibliography{sources.bib}
\end{document}
