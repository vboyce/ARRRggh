\documentclass[]{article}

%opening
\title{ARRR: Why most theory is this area is weak and wrong}
\author{Veronica Boyce, local cynic}

\begin{document}

\maketitle

\section{Sir not appearing open questions}

What's the usefulness of hedging in communication? For reference not for politeness -- what purpose does it serve, when does it create better results (is it comprehension or production side)? 

Why don't things reduce extremely sharply after first success / speed of refinement

What are processing/production ways to get at cost and truthness of descriptions and also how are they sampled? 

%TODO not sure what if anything to say about Eliav


\section{Misc: modularity of language}
different approaches make different claims and assumptions about whether what the thing that's interesting to study is language specifically or communication, with language being the specific case study that comes with a lot of conventions. 

We're coming down on that language should be considered as an instance of communication, and thus, the general things should also hold for other forms of communication (drawing, gesture, etc). But, there's a lot a lot of conventionalized stuff around language in particular that makes it a) an interesting area to study and b) have some complexities that may not exist as much for other modalities. 

\cite{clark1996} at least brings up for me what delimits language from other forms of signalling

This is at group / modality intersection \cite{foxtree2013} looking at different remote communication methods; brings up the possibility of having different modalities at once with different people. In terms of backchannel -- expectations about usage should very based on ease, but also usage should be based on level of *need* to communicate / inverse level of understanding 

channels may differ in their richness and efficiency (some of this might also be habit over different channels)



\subsection{lack of supremacy of certain form of language}
Some of the language studiers also make claims about certain forms of language (oral, face-to-face) being the more correct ones to study, with implications that being the easier, truer medium. 

I disagree -- different modalities and channel set-ups have different affordances. Implications will vary from different set-ups, but even for "naturalistic" there's a whole lot of not face-to-face these days. 

\cite{clark1996} argues that face to face conversation is the more natural (chapter 1) and thus that all other situations of language use will be more challenging. This goes against idea of transfer learning or ability to adapt to other situations. Notably, in the current world, a lot of linguistic communication, even informal communication, takes place not face to face. there's also a strong focus on dyadic interaction in particular, unclear if dyadic is actually a more natural or common situation that others.

\subsection{joint action}
\cite{clark1996} while not the central interest here, theory should be continguous, so we should consider how to consider language when used in conversations with more limited entities. People do talk to their pets and their babies. People also talk with automated system that use language, and now talk with AI chatbots. Are there something totally different? or are there gradients of different actions with some being more prototypically joint and cooperative than others -- at least for conversation with a toddler, it's a conversation, but this suggests that there isn't a clear delimiting between joint actions and not b/c of semi-cooperative situations where one party is not very good at cooperating (pet, baby, AI). 

(generally, insert railing against over splitting approach here)


\section{generally right ideas from Robert (not sure how to classify)}

\cite{hawkins2020b} one question here is how are people so flexible in how they describe things. There's some ad hoc pragmatic reasoning going on, also people have learning mechanisms and can adapt to their partners. In the world, there's new things to experience, so we need ways of using language in new ones (that's just part of the necessary language capacity). There's also going to be new people and contexts, so we have to operate under uncertainty -- we don't know exactly what is shared. Also there's person to person variation in semantics / how they use words. And we can use feedback (verbal and not) to adjust. 

Pragmatics isn't some special thing, these are actually pretty universal. We're closer to living in tangram world than we think. (Except that we usually don't even have such a nice closed class of alternatives) 

Arbitrariness and stability on the semantic side 

\cite{hawkins2020b} How do you break symmetries in initial descriptions: there's prior variability across speaker preferences (they may each have a preferred label, but be unsure if others will accept it) and/or speakers may also not have labels and need to do some sampling. This doesn't account for production time course factors. 

 
\section{Unsatisfying open ?: Groups}

\cite{clark1996} focuses very much on dyadic interaction, and is in the vein that multi-person is serial dialog (verify this last part). 

how to extend ideas of joint action to accomodate multiple people? (re clark1996). 

One question is how you extend RSA up to multiple people -- how to do an utterance designed for multiple people? I guess you could represent separately and fit whatever joint function for optimizing that you want? 

groups allow for more complex structures. \cite{fay2000} tries to look at whether 5 and 10 person groups behave more like dialog or serial monolog (to the group) -- I think it's unclear that this is useful -- how do you define interactions / how do you usefully measure commonalities. There is a suggestion that there may be a transition point in how groups behave, possibly around 10 people, although context and goals seem very likely to push this around 

how to account for side conversations or parallel tracks which are available in some modalities (and not others) also other mediums such as fora / discord / slack have different affordances for allowing branching and parallel things that might be better for groups 

Groups can be complicated \cite{guilbeault2021} looks at how different sizes of groups interacting over a network structure result in different category boundaries, where large groups end up more the same than small groups. Brings up questions about network structure and how we should think about networks even within groups where multiple people are interacting synchronously. Some of the other try to only allow lines between points -- either bidirectional is dialog or single-directional in monolog but there's still the development of shared stuff from the co-presence of listening to someone else even without direct interaction? 

There should be some gradients in terms of these forms of more indirect interaction that aren't explained by dyadic models because the mechanisms that are claimed to support the partner specificity don't cover it (for in part being too low level?)


\section{Misc: Semantics}
One central issue that comes up when discussing linguistic communication is the semantics-pragmatics divide (if you believe in that) or how word meanings are represented. Words certainly have conventionalized meanings (that's how we can talk to people who we haven't talked to before), but there's questions about what the representations are and separately, what an appropriate way to model them is. 

Is literal semantics a useful model? Clearly lots of formal semantics isn't psychologically plausible, but some of the approaches could be useful simplifications for plugging in to RSA. 

What literal semantics will support RSA to explain different things? Like, what if we treated semantics as another Marr Level 1 component of RSA. 

\cite{clark1996}
discussion of the difference between natural signs and signals -- which isn't a distinction I especially buy, but does raise questions over how we determine whether something was an intentional signal from an agent or not 

raises lots of philosophy questions about whether intent is required for something to have meaning and questions about the developmental trajectory of all of this 


\section{Interpersonal / Common ground}

evidently, \cite{hanna2003} has a less crazy definition of CG ? 
\cite{fay2010} calls this being "given" within a pair or community 
Common ground is frequently alluded to, but inconsistency in how the term is used poses problems. It is also usually framed within a non-probabilistic framework. Here I attempt to lay out what sort of common knowledge system is useful. 

As a first gloss, common ground is things that person A expects person B to share and expects B to not be surprised that A thinks it's shared. 

Much ink has been spilled distinguishing common ground with the infinite recursion of knowledge from some spy game. People are pretty bad at holding onto several level recursion. For this theory, it seems worthwhile to think about what forms of private knowledge are common and which are the really hard to keep track of kind. 

It seems reasonable to model people as being able to have up to a couple levels of recursion (with increasing difficulty as recursion increases) or as "many" as that the information is shared and known to be shared. (Aka, most people aren't spies and aren't playing Clue) Whether or not these things are actually shared this way does not matter, the question is a) what the psychological reality is and b) what's reasonable for models. 

Common ground doesn't actually exist (no platonism here) so it always needs to be considered from the viewpoint of a person who can have (implicit) beliefs about others that are used in producing or interpreting utterances. 

This common ground thing is used both at the level of lexicon / interpretation of symbols and also about the specific knowledge about the world and the current state. And also about the temporary ad hoc conventions being built up! 

Whatever this is, it needs to be flexible -- people don't need to be sure that someone else knows. In fact depending on alternatives, backchannel options, stakes, etc, it may sometimes make sense to use descriptions that you don't expect someone else to get, followed by elaborations, because if they do get it, it's the most efficient and this isn't one shot, it's repeated/incremental/interactive. 

\cite{clark1996} contra Clark, many interactions don't require CG -- it's possible to have an interaction with someone who is a lot less aware of things, such as in talking with children, where it's still a conversation, but there may be a lack of cooperation and a dearth of shared territory. 

how to mesh idea of CG with "curse of knowledge" -- CG can't be veridical -- it's a construct and the priors might be messed up by own knowledge (cf people who think that others are modeled on self) -- generally  people may not update knowledge well enough / have incorrect levels of certainty. 

Also for curse of knowledge when people know they're failing to explain, might be a production issue, albeit probably on a conceptual level. 

\cite{clark1996} chapter 4: really one of the populizers of common ground, but issues
- what level is this claim at -- is this a super high level claim or is it about how people approximate what's going on
- what's the developmental trajectory of a proper shared basis
- this blurs various sets of expectations about interlocuters, including language use, other shared cultural stuff, etc etc

we have a lot of expectations about how humans work -- unclear how much of this should be included, because some of it is like an expectation that the mechanism of joint attention (looking at person and looking at object) will make it mutually known. 

what levels of intentionality are assumed here? 

CG and coordination -- what's up with asymmetric exchanges such as adult-child interactions 

you can act like you have CG without having it based on heuristics (and this is probably what kids do, see also their common failures on this) 

even without specific relevant knowledge, \cite{garrison2022} brings up that when there's shared social history (even not about this topic) that makes for more efficient label sharing ; alcohol also gets better labels for strangers, but unclear on mechanism. 

May be cultural mores about how to refer to things or how clear to be that run counter to a pure efficiency model. (Especially if there's perceived social cost to asking questions, might not be on equilibrium). 

\subsection{theory of mind and social representations}
One question is about the partner specificity of representations and how they may bleed together

This also interacts with other things such as ToM all of social cognition and the nature of memory representations 

\subsection{Interactive alignment goofiness}

Newest iteration is from \cite{gandolfi2022} brings up a lot of "control" and monitoring and comparing 

unsure how one is predicting the others utterance without ToM -- is this is behaviorist thing? is the other person not being represented as an agent? or is this just a levels of analysis claim? (representation in general are weird and hard to work with)

I'm confused about this -- and what meta-representing is doing ; feels like a high level description that doesn't have a lot of specific explanatory power

is this really just another "we don't like studying that" -- focuses here a lot more on repair and meta stuff 



\section{Misc: Convention}

Another place there isn't a clear line is between conventions and non-conventionalized coordination. 

Can get coordination from game theory, from probabilistic stuff, don't need to resort to common ground

\section{Meta-commentary}
\subsection{ Levels of analysis and implicit versus explicit knowledge}
many theories are loose on what level of analysis they are addressing things out, and if there posit representations whether they mean that those are actually in the mind somewhere or whether it's a model that might not be actually possesed by people in that form. Also how much there may be a layer of approximation over everything. 

\subsection{experimental space and messiness}
20 	questions with nature. 
the experimental space here is quite huge and many of the experiments lack in generality and rigor (it's hard not to, especially by the standards of the day). Thus, a lot of the theoretical claims, while possibly useful and true, are not well supported by the evidence presented. 

In the start of an area of inquiry, it makes sense to use blunt tools and try to categorize things and limit the "messiness" in order to get any sort of traction and be able to start a theory (at least, this is a common approach, unclear if I actually think that it makes sense to have as much early theory). As evidence accumulates and a landscape of results starts to take shape, this stops cutting it. It becomes clear that there obviously is complexity and ignoring that complexity no longer suggests the right next experiments. Over-simplifying assumptions are common in the theory here, which may have been useful and necessary, but increasing maturity of an area comes with expectation that more be explained, and more quantitatively. 
\section{Misc: Communication}

\cite{clark1986} proposes a try-repair-accept framework where reference expressions don't have to be certain to succeed and can often be present with hedged language indicating their tentative status. If they don't work, there's room for back and forth repair, before a description is accepted by both parties. Acceptance can be implicit in the form of action, or verbally expressed. 

\cite{clark1986} because communication is a joint activity, there is this possibility for repair. In the production of unrehearsed speech, if planning falls through or doesn't stay ahead of production, there may be fillers, restarts, mistakes, etc. But this may still be an ideal efficient path in expectation because it makes use of the ability of the listener to step in and backchannel. It is possible to have production strategy where you keep saying things until they choose the referent. Wouldn't need to calculate a level of informative/overinformativeness because it'll be empirically determined when they choose the thing. 

\cite{clark1986} idea of mutual acceptance is very formal way of putting it -- often won't need to be explicit and joint attention or assumptions of understanding could also work (possible to adapt to systems that due to affordances have different costs of actions and this will effect thresholds -- do you expect verbal response, or are you on a "it's good unless you hear otherwise") 

\cite{clark1986} Will have to look this up again "how does an appeal to prior acceptance work"

%TODO look at \cite{clark1986} more, both notes and original text

\cite{clark1996} idea that language (use) is a joint action

\cite{clark1996} useful take aways: - some communicative acts require a signal from the other person to close the loop (not necessarily communicative) or else there will be continued follow up (or it’ll get dropped) and the type of things that close them vary (basically people look for feedback signals from others)

- there’s on some level some considerations of who interlocuters are and what they know

- people taking into account current circumstances including prior responses ; in general tends to be some sort of monitoring if interactions for various signals when engaged in interactions / joint activities.

\section{Usage/functionalist approach}

\cite{bybee2006} possible that encoding linguistic events into memory and then retrieving is a mechanism that results in efficiency on a language over time scale? (look back at text for more) 
refers to frequency effects -- perhaps some of these are actually predictability effects to link to a more information-theoretic approach 


\section{RSA / probabilistic models }

One point of distinction for a lot of theories is how they handle uncertainty on the part of interlocuters. Many do not and instead contort themselves to accomodate the fact that people make (in these models) assumptions about what others know based on appearance/etc (assuming member of speech community, assuming general knowledge ). But then they also have to specify accomodation and repair mechanisms to deal with when things that were treated as being commonly known where not known by the interlocuter. Especially in reference to hard to describe objects, people may not always describe things in ways that they "know" the other person will understand, which is harder to explain under these stricter models. 

Probabilistic models allow for uncertainty over what other people know, and what others think words mean, etc etc, and thus some of the accomodation is more built in. 

I'm coming out in favor of RSA-style systems -- there are questions about what the right sorts of models and levels of recursion are for the model and there's separately a question of how these get implemented. 

Some key questions here are: what are the alternatives to an utterances? What the literal semantics being used? What's the belief/likelihood representations? 

Gricean maxims provide some descriptive coverage, but there's often (perhaps always) conflict between them and none of them are well enough defined to specify what counts as "relevant" etc. 
\cite{bergen} is mostly trying to explain a very particular set of implicatures raises questions about what types of semantics are a reasonable substrate for RSA. (see notes for details about options for semantics) 

\cite{degen20200406} in choosing a referring expression, some trade offs between cost of utterance and informativeness (also many, many things no where near the frontier, just for the record) 

\cite{degen20200406} uses continuous/soft semantics to explain how redundancy can be useful

could look at \cite{degen20200406} as asking the question: what's a system of semantics and RSA pragmatics that would generate and explain these observed phenomena (what are the constraints or processes that could lead to this output) 

idea of soft semantics is necessary for dealing with tangrams where truth value semantics is unlikely to cut it -- then the specificity and truthiness of the description bits is what's competing 

there's a conflict between compositionality (which \cite{degen20200406} get around by using whole phrase semantics) and incremental RSA / ordering constraints for cross-linguistic --> points to a tension on how to do both

\cite{frank2012a} This is an information theoretic approach, with the goal as to how to quantitatively predict pragmatic inference in context (that's the big goal). Question here is how to do this with less clear referents like abstract things. Features that matter are the contextual salience (although this is sometimes ignored) and a surprisal-based informativeness measure (how much the utterance reduces uncertainty) which could either be empirically or semantic-system calculated. Lots of work is about how to scale this system up to things that eventually come into contact with interesting interactions. Although also may be able to account for some conventionalized pragmatics too. 

\cite{goodman2016} RSA is a computational framework for making quantitative predictions about pragmatic behavior. Variations on RSA pull out different features could be at play -- in some sense a full model would have to include all of these, but many are particularly relevant in different situations and so get modeled in those cases. 

Again a big question is how to ground the semantics of the literal listener -- what usage is based (and what is pragmatic enhancement), and what sort of semantics even works. Truth value doesn't work all the time, how to introduce uncertainty there? 

Some RSA models incorporate cost terms -- abstraction of various things, which is probably related to efficiency pressures (or is modelling the causes of efficiency pressure) 

Questions of how to model uncertainty over word meanings and other parts of interlocuter states (what do they know if this is not mutual knowledge) or their goals

Things that at one point may have been pragmatics become entrenched by usage (possibly within partner specific ad hoc interactions, but certainly over time for a community) 

\cite{goodman2013} this paper again raises the question of what's the right literal meaning to use  -- is it some sort of context free expectation? 

How much do extra-linguistic, domain general factors affect pragmatics? Priors? Seems like there's a fine grained interaction between knowledge and implicature (not really sure what I meant here)

Connection to CG: but how deep does recursion go? Could look at the Franke \& Degen for some of this, but generally seems to be only a couple stages on this recursion, so maybe recursion for CG is also only a couple stages -- not sure how strong the connection is here



\section{Psycholinguistic considerations}
Unsure how these few things fit together, but a few things to consider.

Where the bounded part of bounded rationality comes in -- we expect RSA-style to be a good high level model, but it's not an implementation and is agnostic about what parts are cached and at what level things are cached or short-cutted. 

Another issue is production (in terms of alternative generation, if we think RSA): (leung has a good list for this) As a speaker, one has to come up with a description at all, that description needs to be a good enough fit for the target, and then it also needs to not overly fit competitors. But generating these good enough for target descriptions is non-trivial in some circumstances, and this isn't well-modelled that I know of. How do people generate descriptions in the first place? 

Because this is language, there also seem to be strong habits or expectations about form. Like, there's a level of syntactic well-formedness that's expected, even if it's in some sense inefficient. What's the psycholinguistics here? Is this actually what's fastest to produce because of the habits of mapping to speech? Fastest to interpret? (Syntactic expections for there being a noun come into play in \cite{degen20200406} --> can't say blue without saying pin as well) 

\cite{hanna2003} Looking at the time course of using the speaker's context in processing. Bug questions around whether people are initially egocentric in their interpretations. Makes some weird assumptions about what's required to take other's perspective, on a more mechanistic level, without paying attention to the possibility for some cached heuristics. 

What would an egocentric perspective really be? Language only exists in relation to others (mostly), so how do we think about word meanings in the absence of considering the speaker (clearly there is some way of doing this, "generic speaker", whatever, but I don't know if it's well understood) there's probably also an interesting developmental component -- what's the interplay between language development and ToM development? 

%TODO consider constraint based models 


\section{Efficiency}

\cite{gibson2019} ideas of communicative efficiency, complexity, and learnability- these are all factors that probably shape things. These are going to be context dependent on the usage situations. For in-context efficiency, if there's a lot of uncertainty on what will be an adequate description and there's a strong backchannel or possibility of incremental, then should try things because there's the opportunity for clarification. 

\cite{gibson2019} for various syntactic things, including information locality, these might count as efficiency under the idea of making things maximally easy to understand. This is then going to be how fast can you sufficiently accurately construct the tree from the string, which will be how fast you can convey meaning, but on the comprehenders side? 

seems like what's being minimized is the time to communicate including the production and comprehension processing times (expression length isn't the only thing that's minimized)

Over time an arbitrary form-meaning mapping becomes systematic which then means that the mappings aren't arbitrary because there's embedded in this system and so couldn't change unilaterally. 

\cite{gibson2019} it's hard to measure things like communicative utility which makes it harder to instantiate some of this theory (unclear which things are my thoughts in response and which are brought up in paper) 


Lots of questions about efficiency (see also the notes taken below already)
What are processes that shape efficiency (especially in real-time production)? How does efficiency shape interpretations? There's the long-form for rare bias, but that requires pretty well-formed alternative sets. 

What do we really mean with efficiency? How does it play out on different time scales? What expectations does the overall efficiency framework impose on communication and reference? 

This also gets to ideas of informativeness. And if you get into informativeness and the interaction with relevance. 

\cite{bergen} on how to get matching between costly utterances and rare meanings which is needed for efficiency

see also \cite{zipf1949}


There's a growing consensus that various aspects of language over varied timescales, from evolution of specific language systems to language use, display properties that are parsimoniously explained by efficiency. What does efficiency mean? Something like close to optimal in the trade off between informativity and processing/production. We could see this as a trade-off between different pressures (and that's probably how it developed?), but it could also be close to optimal for how quickly the information content (thought/idea) that a speaker wishes to communicate is conveyed to the listener's thoughts, taking into account that this includes mapping from thoughts to language, producing the language, time to transmit the sound (how fast the speech production is), parsing the signal back into linguistic units, and inferring meaning from that. There's all these (somewhat overlapping) steps, and especially for common messages and key information content, language is far above chance. This could be seen as trade-offs in various things, or as trying to minimize the process subject to some standard of close-enough on the meaning front. 

Zipf is an early proponent of language (the system / artefact) being shaped by some efficiency pressure ("principle of least effort"). He points out that if you're maximizing something, you need to get it all into one equation, you can't optomize two things at once (unless you specify how they trade off with each other, but that's jamming them into one equation). His evidence for this is ... lackluster, and there isn't an attempt at looking at other possible explanations for the data, but his work did inspire others to do a better job at looking at this. 

On a language level, both semantic categories and syntax (and potentially things like phonology as well) seem to be closer to what optimal would be than would be expected by chance. While we can look at this and it looks optimized for efficiency (in different parts of the communication sequence, so addressing different potential bottleneck points). Some of this is that commonly conveyed meanings have short ways of saying them (they have words aligned with those meaning levels) instead of either requiring additional modification (increasing speech time and processing time) or unecessarily overspecifying (increasing processing time). Another component is that syntactic properties such as harmonic word order seem to be good for allowing listeners to generally parse things reasonably. 

There's a question of what the microlevel processes are the drive efficiency, and this is where thinking of things in terms of trade-offs comes in. If something is too hard at any point and causes that to bottleneck it's bad, but things that ease whatever the bottlenecky step are will be more successful and those more widely adopted. There's also the possibility that sources of flexibility become grammaticized as they are reinterpreted (cf. Hawkins), that is soft processing constraints that lead to things like heavy-NP shift may then become seen as part of the grammar by frequent use and then the less frequent parts don't get used even in the specific (rare) situations where they (in isolation) are processing-optimal. There's pressure to grammaticize either from learning or from making parsing easier because fewer options? 

On a more relevant level to present interests, speakers and listeners converse in situations where the language is approximately set in stone. There are expectations about what words mean and how syntax works. Given those inventories, speakers and listeners get to make choices about how to interlocute to efficiently do cooperative mindreading. There's a lot of possible levers here: 

* what wants to be communicated comes with a certain level of precision (or lack thereof). in reference, you just need to get the right target given context. when describing an event, there's parts of it that you want to convey, but it's not recreating everything
* depending on syntax and word order constraints (that is, expectations), different languages lend themselves to pointing out different things. We might imagine that in some circumstances one might want to say isolated words out of order, but this would be pretty unnatural and so incur processing costs on the listener end. Possible also on the speaker end producing it, if planning takes longer. If the channel is artificially limited in a game (or telegram) maybe it's worth this trade off, but in everyday speech, the speech length may not be the limiting part. 
* in most conversational environments, there's the option for the listener to respond or otherwise for feedback to be given. thus, there's the possibility for stopping early if an utterance is sufficient or continuing if it is not (because sufficiency is ascertainable from the listener response either verbally or behaviorally -- i.e. reaching towards the wrong thing). How much this feedback is possible varies, but when it is, it makes sense that utterances can use this. Thus, the most efficient path may be to try something and add more only if needed. (This goes well with not necessarily having absolute knowledge of the interlocuters mental state. )

* over the course of a relationship (or conversation) the conversation history shapes expectations which will allow for different descriptions to become efficient -- either increasing belief that something is enough to try, or making less syntactically normal / highly reduced things seem okay (and thus not incur processing cost?) in context. 

How to deal with that a lot of the time people are joking or whatever? Still trying to convey meaning (even if complex, or meta-linguistic meaning)

As an overarching thing, efficiency is hard to prove or disprove in part because it relies on a lot of linking hypotheses for how it should play out in different domains. But it has parsimony and explanatory value across a lot of levels. It's a good fit to the data, even if the realms for experimental tests are somewhat limited. 

\cite{fay2010} "minimized collaborative effort" (will need to go back and check but notes on: what sorts of interaction define something as being communicative intent, what sort of feedback) use of "givenness" as probably slang for some sort of cultural CG. getting to be more adjacent to some of the network-y stuff that Robert does (but see notes if needed) 


\cite{futrell2022} From an information theory perspective, trying to back out what functional constraints language is operating under / what constraints would explain language. What is language optomizing for and what pressures are driving that optomization? 
\subsection{optim function}

Related to a lot of efficiency work is trying to figure out what the pressures on language are -- like what's the stuff that's being optimized for here, but you have to back it out from behavior. 




\section{informativity}
This is not a well-defined area -- we talk about descriptions being sufficiently informative, but not overly informative, but to make those calculations, you need to have already posited full on semantics and a lack of noise in the interpretation channel. Whatever level of pragmatics you allow, then screws this because now your just barely informative are in expectation overly informative since others can be pragmatically enhanced. Better to explain all of this in terms of RSA where you can deal with it flexibly and not need to specify what the baseline is. 

\cite{baumann2014} argues that in RSA-style environments, speakers often produce "over" informative expressions, and they make the claim that it's because calculating the implicatures is costly.  do listeners ever misinfer b/c of overinformativeness? what's the downside of being "over" informative except from an efficiency stand-point? how do we even define "over" specification, since this is always going to be in regard to a level of inference and a level of probabilistic certainty that we're okay with. 

\cite{bergen} on specificity implicatures will also raise questions here, since do you calculate informativeness before or after the implicature? 

From efficiency if production time speed if a big factor, then it may be overall efficient. 

\cite{degen20200406} on informativity as being the relevant thing *not* some technical definition of overspecification 

idea of "pragmatic surprisal" 

\section{"""Alignment"""}

\cite{eliav2023} (maybe?) brings up the idea that conceptual alignment could be one to one or it could be a wider thing that displays transfer learning (concepts are on different levels) 

not sure where it goes: but there seems to be good explanations for why using multiple chunks might be good b/c uncertain if other person will understand (although see open questions about how true this is) 

\section{Unsatisfying ?: Explaining reduction phenomena}
This is one of the big puzzles -- it makes sense from an efficiency standpoint, but most theories are better at explaining why you should form and maintain a pact and less good at how it should crystallize into something shorter (in this sense, changing). 

Robert's point about having multi-symbol things and how usage then refines the meaning us one approach. Still a question of modelling the rate of drop-off. 

Also seems like there's some probabilstic path dependency here, possibly related to relationships or humor value

we may be able to identify what types of descriptions/utterances tend to work out (holistic/analogic ones re: \cite{clark1986}) but they aren't absolute and sometimes ones that picked up on very different features are ones that end up working -- there's a wide variety of what can work + some strong preferences (so probably some distribution and priors) 

\section{General approach of what I think is right}
Boundedly rational approach to communication. Can be modeled on a Marr level 1 by some sort of RSA approach that probabilistic and admits to malleability by non and extra-linguistic factors. A full account would need many chunks, since the back and forth and incremental isn't well developed in that system. 

At other levels, there's questions about how production works here and that may be a constraining factor that contributed to the approximateness if not everything is easy to produce quickly. 

May resemble constraint-based in terms of optimizing with a bunch of soft constraints over what will or won't work well. 

\section{Constraints from other disciplines}
We're not making a theory of everything here, but if there are going to be appeals to parsimony, they need to take into account all the other stuff that will perforce be going on outside of this. Thus, if some things already are needed or well-explained that's to be  taken into account, especially with regard to domain general processes. 

In particular, reference contacts other disciplines in the forms of social cognition, since this is a social interaction and involves tracking other's understanding and knowledge states, and memory, since it involves remembering things. What we know and belief from these disciplines will need to match. Thus, if something is common and considered a solved problem, we can't appeal to it being hard. On the other hand, memory is decently understood and so we should reject models that posit implausible memory systems or memory retrieval operations. 

Models may be skimpy on these fronts if they are high level, but that should be explicit. 

Other evidence from language use and perception/ categorization boundaries may also come into play. 

%TODO figure out what my not how memory works coment on Clark chapter 4 was about 

\section{List of sources of disagreement}

Memory format 

Ease of social reasoning (note that social cognition says that people are pretty good at a bunch of this and that it appears early in development and is quite advanced and may be a major factor in being human, so theories that posit that this is super hard and requires tool-level thinking are probably confused, or at least not getting parsimony points from this)

Explanation for reduction

Ego-centrism / time course of listeners restricting to CG (if that even makes sense)

Monolog / dialog /etc 

Is language privileged and if so, what specific instances of it

Rules v probabilistic

Is pragmatic inference an always thing? Role of context in linguistic interpretation (yeah, probably always there, any restrictions would have to show up in processing and would be hard to measure? could throw around surprisal with non-linguistic context, but idk if that's been studied) 









\section{old stuff below here}





\section{Why common ground is dumb}

The idea of common ground assumes that it's a thing that exists. That isn't true but also isn't relevant -- what matters is what one person thinks is sufficiently mutual between them and their interlocuter. The depth effect of common ground is rarely relevant, but people can be mistaken about what is in common (ex. you think it's in common ground because you told it to them, but actually they forgot). Also, as will be discussed extensively, this isn't all or nothing -- I can have uncertainty over who knows what, or think something will be understood because it's generally known by people without specific reason for the individual. 

The question of what are your priors or their priors and how tailored are they to the current context and in particular how much theory of mind and perspective taking (aka recursion) there is, is relevant. And that's vaguely what's being said, but with the mechanisms probably mostly wrong. 

One issue with distinguishing what shapes utterance production is that we primarily have access to the outcome at the end, which mostly isn't what anyone is arguing about. The questions are about what the process is for generating that utterance and what sorts of things are how prominent in the process when. This leads to lots of hand-wavy models about how language is produced (again, hard to model well since we don't have access) and flimsy linking assumptions with behavior. 

One assumption is that telling someone to respond faster (time pressure) will change the production pathway/process in a way that limits later steps more than earlier steps. Clearly, time pressure (and other resource constraints) do affect production, but how is going to be a lot more complicated, especially given the lack of good production model. 

Horton \& Keysar are trying to adjudicate between: initial planning takes into account common ground, and initial planning does not taken into account common ground, but monitor \& fix does (and does so pre utterance initiation sometimes) 

Everything is probably more continuous than these things posit. It's a probabilistic world of bounded rationality and heuristics to fill in gaps. 

egocentrism: what is seen as egocentric behavior could be either a) not caring whether the other person understands / could be referring to a thing, b) mistaken beliefs about what the other person knows , or c) satisficing under limited resources producing either (which isn't really a third option) 

the people who call it egocentrism don't call it that when people make mistakes about what is in common ground. But as \cite{hanna2003} point out, you can never actually know what another person knows 

the ways things are said to get into common ground aren't consistent with various general assumptions that people know things (like default assumption of sharing the same language, or knowing basic facts, etc) or inferences (went to same college, is a local, etc). 

common ground is mostly useful when discussing spying -- something like expected to be comprehensible to listener based on some set of things makes more sense 

\section{name versus description}

how descriptions become names

using names with descriptions as a teaching thing (to make future things faster) 

overinformativity -- what's wrong with it? 

\section{Points of disagreement}

general tension between people who want a set of absolutes, versus gradients

how difficult doing theory of mind is (and under what circumstances)

time course of incorporation of interlocuter specific information 

interpretation of eye movements

what appropriate levels of naturalism are

what work is on the speaker v listener

"""reference diaries"""

terms I don't like: common ground, entrain, aim low v aim high v aim average

Who adjusts how much: do speakers design and/or do listeners accomodate


\section{general points of agreement}

under some circumstances, people will do conscious reasoning like they're playing clue and use very explicit theory of mind 

some of the time, people will not fully map everything and will instead need to rely on short-circuits and proxies 

the question is when these times are and what happens in "normal" situations in the middle ... 


\section{Vaguely plausible}

One key distinction that doesn't get explicitly highlighted much (but is brought up in some recent work, such as \cite{leung2023}) is that the process of `first contact' -- establishing initial reference -- is at least somewhat separable from the repeated reference / reduction phenomenon. They're probably not totally separate as the same systems will need to work for both, but there are probably peculiarities to the reduction part. 

Factors affecting how utterances are produced and interpreted are going to be varied and probabilistic. For instance, we could treat this as some sort of salience prior, where you expect the interlocuter to *probably* refer to the thing that is mutually seen (see "common ground") but the chance of them referring to something you can see and then can't isn't 0 (and thus can be increased in the likelihood). 

Considerations from audience design literature: generally pretty reasonable, communication is in some high dimensional space and there are a lot of potential influences. 

``partial pooling'' model: some sensitivity to context \& who said what, but source memory is hard. Also, there are recency effects \& some expected transfer/universals (learning language) 

Distinction between elaborations (common with a new listener) \& totally new ideas (rare)

From audience design: speakers take into account the whole set of listeners who are present -- tend to tailor so that all could understand, but use meanings/shorthands that knowledgeable ones will recognize (=> many possible ways to accomodate multiple listeners, perspectives, also consistent with a name \& describe that's good for future) 

performance is constrained by realities of task switching \& memory and interference (so some performance things that don't seem ideal may be from those things) -- bounded rationality 

speakers can also take listener context into account (at least when known) but again probably bounded by working memory

shared social history makes for efficient label sharing 


\section{meta}
general meta-theory: when something is a new area, easy to make over-simplifiying models of “here are the two or three ways it could work” this is useful, but as a field matures, important to accept nuances.

useful sketch of some options versus reality / full messy space of options

Stronger if you have gradients (ideal if you have math model and can do something quantitative)

\section{}

my concerns about whether speakers are calibrated/optimized or not continue (see also alcohol paper) 

\section{zipf}
Could be generously interpreted as an early precedent to bounded rationality / resource rationality 

points out need to be very clear about what is being optomized

Efficiency depends on what the time scale that's being optomized for is -- someitmes more prep / longer will pay off over repetition (and you don't know the future, so probabilistic)

gives an ahistorical account looking at word distributions and word \ meaning distributions 

some sort of utility access model ? 
\section{To explore further}

What's the SOTA in language production modelling? Utterance planning? 

Constraint based accounts of production (and comprehension) see \cite{hanna2003} highlights for suggested readings

maybe grice

forward search \cite{heller2012} since they're pretty reasonable about the idea of name versus what properties to include 

piantadosi on power laws and how they come from everything 

see various yoon and brown schmidt for highlighted citations to read 
\bibliography{sources.bib}
\end{document}
